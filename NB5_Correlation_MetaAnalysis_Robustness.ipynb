{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB5 — Correlation, Meta-Analysis & Robustness\n",
    "\n",
    "Donor-level Spearman correlations, DerSimonian-Laird random-effects meta-analysis, leave-one-dataset-out (LODO) sensitivity, housekeeping gene robustness, and GR-stratified differential expression.\n",
    "\n",
    "**Paper:** Zafar SA, Qin W. *Thymus-Derived Myeloid Education Signatures Predict Microglial Tolerance Positioning and Are Modulated by Glucocorticoid Stress-Axis Activity.* Neuroimmunomodulation (2026).\n",
    "\n",
    "> **Note:** Update the path variables in section 0 to match your local directory structure before running. Raw data can be obtained from the public repositories listed in Supplementary Table S1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, time, math, json, gc, glob, warnings, traceback\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 9,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 1200,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.05,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"xtick.major.width\": 0.6,\n",
    "    \"ytick.major.width\": 0.6,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"ticks\")\n",
    "    HAS_SNS = True\n",
    "except ImportError:\n",
    "    HAS_SNS = False\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    HAS_SM = True\n",
    "except ImportError:\n",
    "    HAS_SM = False\n",
    "\n",
    "# User config\n",
    "np.random.seed(42)\n",
    "\n",
    "PROC_DIR  = Path(\".\") / \"data\" / \"processed\" / \"aim2_microglia\"  # <-- SET PATH\n",
    "MANUS_DIR = Path(\".\") / \"outputs\" / \"manuscript\"  # <-- SET PATH\n",
    "FIG_DIR   = MANUS_DIR / \"Figures\"\n",
    "TAB_DIR   = MANUS_DIR / \"Tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NB4_TABLE_DIR = TAB_DIR\n",
    "SCORED_GLOB   = str(PROC_DIR / \"*__microglia_scored.h5ad\")\n",
    "\n",
    "MAX_CELLS_FOR_CELL_DE        = 50000\n",
    "MAX_CELLS_FOR_CELL_RECOMPUTE = 50000\n",
    "DPI      = 1200\n",
    "N_BOOT   = 2000\n",
    "N_PERM   = 5000\n",
    "ALPHA    = 0.05\n",
    "\n",
    "RUN_DIAGNOSTICS_ONLY = False\n",
    "DO_ENRICHMENT        = False\n",
    "\n",
    "# Color palettes\n",
    "PAL_DATASETS = sns.color_palette(\"Set2\", 8) if HAS_SNS else plt.cm.Set2(np.linspace(0, 1, 8))\n",
    "PAL_GR       = {\"GR_high\": \"#D6604D\", \"GR_low\": \"#4393C3\"}\n",
    "\n",
    "\n",
    "# Small utilities\n",
    "def _now():  return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def _rss_gb():\n",
    "    try:\n",
    "        import psutil\n",
    "        return psutil.Process(os.getpid()).memory_info().rss / (1024**3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def _cpu_pct():\n",
    "    try:\n",
    "        import psutil\n",
    "        return psutil.Process(os.getpid()).cpu_percent(interval=None)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[{_now()}] {msg}\", flush=True)\n",
    "\n",
    "def fmt_td(seconds: float) -> str:\n",
    "    seconds = max(0.0, float(seconds))\n",
    "    m, s = divmod(int(seconds), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h}:{m:02d}:{s:02d}\" if h > 0 else f\"{m}:{s:02d}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Progress:\n",
    "    label: str\n",
    "    total: int\n",
    "    start_t: float = None\n",
    "    done: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.start_t = time.time() if self.start_t is None else self.start_t\n",
    "\n",
    "    def tick(self, inc=1):\n",
    "        self.done = min(self.done + int(inc), self.total)\n",
    "\n",
    "    def line(self, extra: str = \"\") -> str:\n",
    "        elapsed = time.time() - self.start_t\n",
    "        frac = (self.done / self.total) if self.total > 0 else 1.0\n",
    "        eta  = elapsed / max(frac, 1e-9) - elapsed if self.total > 0 else 0.0\n",
    "        return (f\"PROG {self.label}: {self.done}/{self.total} ({frac*100:.1f}%) | \"\n",
    "                f\"elapsed={fmt_td(elapsed)} | ETA={fmt_td(eta)} | \"\n",
    "                f\"cpu={_cpu_pct():.1f}% | rss={_rss_gb():.2f}GB\"\n",
    "                + (f\" | {extra}\" if extra else \"\"))\n",
    "\n",
    "\n",
    "class Heartbeat:\n",
    "    def __init__(self, label: str, every_s: int = 30):\n",
    "        self.label = label\n",
    "        self.every_s = int(every_s)\n",
    "        self._t0 = None\n",
    "        self._running = False\n",
    "        self._last_msg = \"\"\n",
    "        self._thread = None\n",
    "\n",
    "    def start(self, msg: str = \"start\"):\n",
    "        import threading\n",
    "        self._t0 = time.time()\n",
    "        self._running = True\n",
    "        log(f\"HB {self.label}: {msg}\")\n",
    "        def _loop():\n",
    "            while self._running:\n",
    "                elapsed = time.time() - self._t0\n",
    "                log(f\"HB {self.label}: running | elapsed={fmt_td(elapsed)} | \"\n",
    "                    f\"cpu={_cpu_pct():.1f}% | rss={_rss_gb():.2f}GB\"\n",
    "                    + (f\" | {self._last_msg}\" if self._last_msg else \"\"))\n",
    "                time.sleep(self.every_s)\n",
    "        self._thread = threading.Thread(target=_loop, daemon=True)\n",
    "        self._thread.start()\n",
    "\n",
    "    def update(self, msg: str):\n",
    "        self._last_msg = str(msg)\n",
    "\n",
    "    def stop(self, msg: str = \"done\"):\n",
    "        self._running = False\n",
    "        elapsed = time.time() - (self._t0 or time.time())\n",
    "        log(f\"HB {self.label}: {msg} | elapsed={fmt_td(elapsed)}\")\n",
    "\n",
    "\n",
    "def save_fig(path: Path, fig=None):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fig is not None:\n",
    "        fig.savefig(path, dpi=DPI, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.savefig(path, dpi=DPI, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    log(f\"SAVED FIG {path}\")\n",
    "\n",
    "\n",
    "def save_xlsx(df: pd.DataFrame, path: Path, sheet_name=\"Sheet1\"):\n",
    "    if df is None: df = pd.DataFrame()\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as w:\n",
    "        df.to_excel(w, index=False, sheet_name=sheet_name)\n",
    "    log(f\"SAVED TABLE {path}\")\n",
    "\n",
    "\n",
    "def save_xlsx_multi(dfs: Dict[str, pd.DataFrame], path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as w:\n",
    "        for name, df in dfs.items():\n",
    "            if df is not None and not df.empty:\n",
    "                df.to_excel(w, index=False, sheet_name=name[:31])\n",
    "    log(f\"SAVED TABLE (multi-sheet) {path}\")\n",
    "\n",
    "\n",
    "def sig_stars(p):\n",
    "    if pd.isna(p) or not np.isfinite(p): return \"ns\"\n",
    "    if p < 0.001: return \"***\"\n",
    "    if p < 0.01:  return \"**\"\n",
    "    if p < 0.05:  return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "\n",
    "# Statistical toolkit \n",
    "\n",
    "def safe_numeric(x) -> np.ndarray:\n",
    "    return pd.to_numeric(pd.Series(x), errors=\"coerce\").to_numpy()\n",
    "\n",
    "def _align_finite(x, y, min_n=5):\n",
    "    x = safe_numeric(x); y = safe_numeric(y)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if m.sum() < min_n: return None, None, 0\n",
    "    return x[m], y[m], int(m.sum())\n",
    "\n",
    "\n",
    "def corr_with_pvalue(x, y, method=\"spearman\", min_n=5, n_boot=N_BOOT):\n",
    "    \"\"\"Correlation with p-value + bootstrap 95% CI.\"\"\"\n",
    "    xa, ya, n = _align_finite(x, y, min_n)\n",
    "    out = {\"r\": np.nan, \"p\": np.nan, \"n\": n,\n",
    "           \"ci_lo\": np.nan, \"ci_hi\": np.nan, \"method\": method}\n",
    "    if xa is None: return out\n",
    "    if method == \"spearman\":\n",
    "        r, p = scipy_stats.spearmanr(xa, ya)\n",
    "    elif method == \"pearson\":\n",
    "        r, p = scipy_stats.pearsonr(xa, ya)\n",
    "    else:\n",
    "        r, p = scipy_stats.spearmanr(xa, ya)\n",
    "    out[\"r\"] = float(r); out[\"p\"] = float(p); out[\"n\"] = n\n",
    "    rng = np.random.RandomState(42)\n",
    "    boots = np.full(n_boot, np.nan)\n",
    "    for i in range(n_boot):\n",
    "        idx = rng.randint(0, n, n)\n",
    "        if method == \"spearman\":\n",
    "            boots[i] = scipy_stats.spearmanr(xa[idx], ya[idx])[0]\n",
    "        else:\n",
    "            boots[i] = scipy_stats.pearsonr(xa[idx], ya[idx])[0]\n",
    "    fb = boots[np.isfinite(boots)]\n",
    "    if len(fb) > 10:\n",
    "        out[\"ci_lo\"] = float(np.percentile(fb, 2.5))\n",
    "        out[\"ci_hi\"] = float(np.percentile(fb, 97.5))\n",
    "    return out\n",
    "\n",
    "\n",
    "def partial_corr(df, x, y, covars, method=\"spearman\", min_n=10):\n",
    "    \"\"\"Partial correlation controlling for covariates.\"\"\"\n",
    "    cols = [x, y] + covars\n",
    "    d = df[[c for c in cols if c in df.columns]].dropna()\n",
    "    actual_covars = [c for c in covars if c in d.columns]\n",
    "    if d.shape[0] < min_n or not HAS_SM or len(actual_covars) == 0:\n",
    "        return corr_with_pvalue(df.get(x, []), df.get(y, []), method=method, min_n=min_n)\n",
    "    C = sm.add_constant(d[actual_covars].values)\n",
    "    resid_x = sm.OLS(d[x].values, C).fit().resid\n",
    "    resid_y = sm.OLS(d[y].values, C).fit().resid\n",
    "    out = corr_with_pvalue(resid_x, resid_y, method=method, min_n=min_n)\n",
    "    out[\"partial\"] = True\n",
    "    out[\"covariates\"] = \",\".join(actual_covars)\n",
    "    return out\n",
    "\n",
    "\n",
    "def bh_fdr(pvals) -> np.ndarray:\n",
    "    p = np.asarray(pvals, float)\n",
    "    q = np.full_like(p, np.nan)\n",
    "    m = np.isfinite(p)\n",
    "    if m.sum() == 0: return q\n",
    "    if HAS_SM:\n",
    "        _, qv, _, _ = multipletests(p[m], method=\"fdr_bh\")\n",
    "        q[m] = qv\n",
    "    else:\n",
    "        pv = p[m]; nn = pv.size\n",
    "        order  = np.argsort(pv)\n",
    "        ranked = pv[order]\n",
    "        qv = ranked * nn / (np.arange(nn) + 1.0)\n",
    "        qv = np.minimum.accumulate(qv[::-1])[::-1]\n",
    "        q[m] = qv[np.argsort(order)]\n",
    "    return q\n",
    "\n",
    "\n",
    "# Column detection helpers\n",
    "\n",
    "MES_COLS = [\n",
    "    \"MES01_score\", \"MES02_score\", \"MES03_score\", \"MES04_score\",\n",
    "    \"MES05_score\", \"MES06_score\", \"MES07_score\", \"MES08_score\",\n",
    "]\n",
    "GR_COL = \"GR_composite\"\n",
    "\n",
    "\n",
    "def detect_donor_col(adata):\n",
    "    cols = list(map(str, adata.obs.columns))\n",
    "    for cand in [\"donor_id\",\"Donor ID\",\"donor\",\"DonorID\",\"patient_id\",\"PatientID\",\"subject_id\",\"individual\"]:\n",
    "        if cand in cols: return cand\n",
    "    low = {c: c.lower() for c in cols}\n",
    "    for c in cols:\n",
    "        s = low[c]\n",
    "        if \"donor\" in s and (\"id\" in s or s.endswith(\"donor\")): return c\n",
    "    for c in cols:\n",
    "        s = low[c]\n",
    "        if (\"patient\" in s or \"subject\" in s or \"individual\" in s) and \"id\" in s: return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_mes_cols(obs):\n",
    "    cols = list(map(str, obs.columns))\n",
    "    if all(c in cols for c in MES_COLS): return MES_COLS[:]\n",
    "    pat = re.compile(r\"^MES0?[1-8](_score)?$\", re.IGNORECASE)\n",
    "    return [c for c in cols if pat.match(c)]\n",
    "\n",
    "\n",
    "def detect_gr_col(obs):\n",
    "    cols = list(map(str, obs.columns))\n",
    "    if GR_COL in cols: return GR_COL\n",
    "    low = {c: c.lower() for c in cols}\n",
    "    for c in cols:\n",
    "        s = low[c]\n",
    "        if \"gr\" in s and (\"composite\" in s or \"score\" in s): return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_sex_col(obs):\n",
    "    cols = list(map(str, obs.columns))\n",
    "    for cand in [\"Sex\",\"sex\",\"gender\",\"Gender\"]:\n",
    "        if cand in cols: return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_pmi_col(obs):\n",
    "    cols = list(map(str, obs.columns))\n",
    "    for cand in [\"PMI\",\"pmi\",\"post_mortem_interval\",\"PostMortemInterval\"]:\n",
    "        if cand in cols: return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_batch_col(obs):\n",
    "    cols = list(map(str, obs.columns))\n",
    "    for cand in [\"batch\",\"Batch\",\"library_prep_batch\",\"sequencing_batch\"]:\n",
    "        if cand in cols: return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def canonical_mes_label(col: str) -> str:\n",
    "    c = str(col).replace(\"_score\", \"\").replace(\"Score\", \"\").strip()\n",
    "    m = re.match(r\"^(MES0?[1-8])$\", c, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        x = m.group(1).upper()\n",
    "        if len(x) == 4: x = x[:3] + \"0\" + x[3:]\n",
    "        return x\n",
    "    return c\n",
    "\n",
    "\n",
    "# Covariate builder for donor-level DataFrames\n",
    "\n",
    "def build_covariates(donor_df, obs, donor_col):\n",
    "    \"\"\"Detect + encode covariates into donor_df.  Returns covariate column names.\"\"\"\n",
    "    covars = []\n",
    "    sx  = detect_sex_col(obs)\n",
    "    pmi = detect_pmi_col(obs)\n",
    "    bat = detect_batch_col(obs)\n",
    "\n",
    "    if sx and sx in obs.columns:\n",
    "        sex_map = obs.groupby(obs[donor_col].astype(str))[sx].first()\n",
    "        donor_df[\"_cov_sex_num\"] = donor_df.index.map(\n",
    "            lambda d: 0.0 if str(sex_map.get(d, \"\")).lower().startswith(\"m\")\n",
    "                      else (1.0 if str(sex_map.get(d, \"\")).lower().startswith(\"f\") else np.nan))\n",
    "        if donor_df[\"_cov_sex_num\"].notna().sum() > 5:\n",
    "            covars.append(\"_cov_sex_num\")\n",
    "\n",
    "    if pmi and pmi in obs.columns:\n",
    "        pmi_map = obs.groupby(obs[donor_col].astype(str))[pmi].mean(numeric_only=True)\n",
    "        donor_df[\"_cov_pmi\"] = donor_df.index.map(lambda d: pmi_map.get(d, np.nan))\n",
    "        donor_df[\"_cov_pmi\"] = pd.to_numeric(donor_df[\"_cov_pmi\"], errors=\"coerce\")\n",
    "        if donor_df[\"_cov_pmi\"].notna().sum() > 5:\n",
    "            covars.append(\"_cov_pmi\")\n",
    "\n",
    "    if bat and bat in obs.columns:\n",
    "        bat_map = obs.groupby(obs[donor_col].astype(str))[bat].first()\n",
    "        u = bat_map.dropna().unique()\n",
    "        if 1 < len(u) < 20:\n",
    "            donor_df[\"_cov_batch_num\"] = donor_df.index.map(\n",
    "                lambda d: pd.Categorical([bat_map.get(d, np.nan)]).codes[0])\n",
    "            donor_df[\"_cov_batch_num\"] = donor_df[\"_cov_batch_num\"].replace(-1, np.nan).astype(float)\n",
    "            if donor_df[\"_cov_batch_num\"].notna().sum() > 5:\n",
    "                covars.append(\"_cov_batch_num\")\n",
    "    return covars\n",
    "\n",
    "\n",
    "# Data integrity utilities\n",
    "\n",
    "def ensure_log1p_X(adata, prefer_layer_counts=\"counts\"):\n",
    "    def _maxX(a):\n",
    "        return float(a.X.max()) if not sp.issparse(a.X) else float(a.X.max())\n",
    "    try:\n",
    "        mx = _maxX(adata)\n",
    "    except Exception:\n",
    "        mx = None\n",
    "    if mx is not None and mx > 30:\n",
    "        if prefer_layer_counts in adata.layers:\n",
    "            X = adata.layers[prefer_layer_counts]\n",
    "            tmp = ad.AnnData(X=X, obs=adata.obs.copy(), var=adata.var.copy())\n",
    "            sc.pp.normalize_total(tmp, target_sum=1e4)\n",
    "            sc.pp.log1p(tmp)\n",
    "            adata.X = tmp.X\n",
    "        else:\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "    else:\n",
    "        if not sp.issparse(adata.X):\n",
    "            adata.X = np.asarray(adata.X)\n",
    "\n",
    "\n",
    "def stratified_subsample_by_donor_positions(obs, donor_col, max_cells=50000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    donors = obs[donor_col].astype(str).values\n",
    "    n = donors.shape[0]\n",
    "    if n <= max_cells: return np.arange(n, dtype=int)\n",
    "    uniq = pd.unique(donors)\n",
    "    per = max(10, int(np.floor(max_cells / max(len(uniq), 1))))\n",
    "    pos_all = []\n",
    "    total = 0\n",
    "    for d in uniq:\n",
    "        pos = np.flatnonzero(donors == d)\n",
    "        if pos.size == 0: continue\n",
    "        take = min(per, pos.size)\n",
    "        choice = rng.choice(pos, size=take, replace=False)\n",
    "        pos_all.append(choice)\n",
    "        total += choice.size\n",
    "        if total >= max_cells: break\n",
    "    pos = np.concatenate(pos_all) if pos_all else rng.choice(np.arange(n), size=max_cells, replace=False)\n",
    "    if pos.size > max_cells:\n",
    "        pos = rng.choice(pos, size=max_cells, replace=False)\n",
    "    return pos.astype(int)\n",
    "\n",
    "\n",
    "def add_gr_group(adata, gr_col, out_col=\"GR_group\"):\n",
    "    if gr_col not in adata.obs.columns: return\n",
    "    x = pd.to_numeric(adata.obs[gr_col], errors=\"coerce\").values\n",
    "    finite = np.isfinite(x)\n",
    "    if finite.sum() == 0:\n",
    "        adata.obs[out_col] = pd.Categorical([\"NA\"]*adata.n_obs, categories=[\"GR_low\",\"GR_high\",\"NA\"])\n",
    "        return\n",
    "    med = float(np.nanmedian(x[finite]))\n",
    "    grp = np.where(x >= med, \"GR_high\", \"GR_low\")\n",
    "    grp = np.where(finite, grp, \"NA\")\n",
    "    adata.obs[out_col] = pd.Categorical(grp, categories=[\"GR_low\",\"GR_high\",\"NA\"])\n",
    "\n",
    "\n",
    "# Scanpy DE helper (PATCH 4 + pseudobulk-friendly)\n",
    "\n",
    "def de_rank_genes_groups(adata, group_col, group1=\"GR_high\", group0=\"GR_low\",\n",
    "                         method=\"wilcoxon\", n_genes=None, min_n_per_group=10):\n",
    "    if group_col not in adata.obs.columns: return pd.DataFrame()\n",
    "    vc = adata.obs[group_col].astype(str).value_counts(dropna=False)\n",
    "    if int(vc.get(group1, 0)) < min_n_per_group or int(vc.get(group0, 0)) < min_n_per_group:\n",
    "        return pd.DataFrame()\n",
    "    mask = adata.obs[group_col].astype(str).isin([group0, group1]).values\n",
    "    if int(mask.sum()) < max(2 * min_n_per_group, 10):\n",
    "        return pd.DataFrame()\n",
    "    aa = adata[mask, :].copy()\n",
    "    ensure_log1p_X(aa, prefer_layer_counts=\"counts\")\n",
    "    sc.tl.rank_genes_groups(aa, groupby=group_col, groups=[group1],\n",
    "                            reference=group0, method=method,\n",
    "                            n_genes=n_genes if n_genes else aa.n_vars,\n",
    "                            use_raw=False)\n",
    "    res = aa.uns.get(\"rank_genes_groups\", None)\n",
    "    if res is None: return pd.DataFrame()\n",
    "\n",
    "    def _extract(x):\n",
    "        if x is None: return None\n",
    "        try:\n",
    "            if isinstance(x, dict): return np.array(x.get(group1, np.nan))\n",
    "        except: pass\n",
    "        try:\n",
    "            if hasattr(x, \"dtype\") and getattr(x.dtype, \"names\", None):\n",
    "                if group1 in x.dtype.names: return np.array(x[group1])\n",
    "        except: pass\n",
    "        try:\n",
    "            arr = np.array(x)\n",
    "            if arr.ndim == 2:\n",
    "                if arr.shape[1] == 1: return arr[:, 0]\n",
    "                if arr.shape[0] == 1: return arr[0, :]\n",
    "        except: pass\n",
    "        try: return np.array(x)\n",
    "        except: return None\n",
    "\n",
    "    genes = _extract(res.get(\"names\"))\n",
    "    if genes is None: return pd.DataFrame()\n",
    "    df = pd.DataFrame({\n",
    "        \"gene\":  pd.Series(genes).astype(str),\n",
    "        \"logFC\": pd.to_numeric(pd.Series(_extract(res.get(\"logfoldchanges\"))), errors=\"coerce\"),\n",
    "        \"score\": pd.to_numeric(pd.Series(_extract(res.get(\"scores\"))), errors=\"coerce\"),\n",
    "        \"pval\":  pd.to_numeric(pd.Series(_extract(res.get(\"pvals\"))), errors=\"coerce\"),\n",
    "        \"padj\":  pd.to_numeric(pd.Series(_extract(res.get(\"pvals_adj\"))), errors=\"coerce\"),\n",
    "    })\n",
    "    return df.dropna(subset=[\"gene\"]).drop_duplicates(subset=[\"gene\"])\n",
    "\n",
    "\n",
    "# Diagnostics\n",
    "\n",
    "def diagnostics():\n",
    "    rows = []\n",
    "    rows.append((\"PROC_DIR exists\", PROC_DIR.exists()))\n",
    "    rows.append((\"PROC_DIR\", str(PROC_DIR)))\n",
    "    rows.append((\"MANUS_DIR exists\", MANUS_DIR.exists()))\n",
    "    rows.append((\"SCORED_GLOB\", SCORED_GLOB))\n",
    "    files = sorted(glob.glob(SCORED_GLOB))\n",
    "    rows.append((\"scored matches\", len(files)))\n",
    "    if files:\n",
    "        rows.append((\"first scored\", Path(files[0]).name))\n",
    "        rows.append((\"last scored\",  Path(files[-1]).name))\n",
    "        try:\n",
    "            a = sc.read_h5ad(files[0], backed=None)\n",
    "            if not a.obs_names.is_unique: a.obs_names_make_unique()\n",
    "            if not a.var_names.is_unique: a.var_names_make_unique()\n",
    "            rows.append((\"detected GR col\",    detect_gr_col(a.obs) or \"NONE\"))\n",
    "            rows.append((\"detected MES cols\",  \",\".join(detect_mes_cols(a.obs)) or \"NONE\"))\n",
    "            rows.append((\"detected donor col\", detect_donor_col(a) or \"NONE\"))\n",
    "            rows.append((\"detected sex col\",   detect_sex_col(a.obs) or \"NONE\"))\n",
    "            rows.append((\"detected PMI col\",   detect_pmi_col(a.obs) or \"NONE\"))\n",
    "            rows.append((\"detected batch col\", detect_batch_col(a.obs) or \"NONE\"))\n",
    "        except Exception as e:\n",
    "            rows.append((\"peek error\", repr(e)))\n",
    "    df = pd.DataFrame(rows, columns=[\"check\", \"value\"])\n",
    "    save_xlsx(df, TAB_DIR / \"NB5_DIAG.xlsx\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load scored cohorts (PATCH 1: unique names)\n",
    "\n",
    "def load_nb4_tables():\n",
    "    tables = {}\n",
    "    for k, p in [(\"Main_Table5\",          NB4_TABLE_DIR/\"Main_Table5.xlsx\"),\n",
    "                 (\"Main_Table1\",          NB4_TABLE_DIR/\"Main_Table1.xlsx\"),\n",
    "                 (\"Supplementary_Table1\", NB4_TABLE_DIR/\"Supplementary_Table1.xlsx\"),\n",
    "                 (\"Supplementary_Table2\", NB4_TABLE_DIR/\"Supplementary_Table2.xlsx\"),\n",
    "                 (\"Supplementary_Table3\", NB4_TABLE_DIR/\"Supplementary_Table3.xlsx\"),\n",
    "                 (\"Supplementary_Table5\", NB4_TABLE_DIR/\"Supplementary_Table5.xlsx\")]:\n",
    "        if p.exists():\n",
    "            try: tables[k] = pd.read_excel(p)\n",
    "            except: pass\n",
    "    return tables\n",
    "\n",
    "\n",
    "def load_scored_cohorts():\n",
    "    files = sorted(glob.glob(SCORED_GLOB))\n",
    "    log(f\"[NB5] Found scored cohorts: {[Path(x).name for x in files]}\")\n",
    "    pr = Progress(\"Load cohorts\", total=len(files))\n",
    "    hb = Heartbeat(\"load_scored_cohorts\", every_s=15)\n",
    "    hb.start(\"start\")\n",
    "    cohorts = {}\n",
    "    for f in files:\n",
    "        a = sc.read_h5ad(f)\n",
    "        if not a.obs_names.is_unique: a.obs_names_make_unique()\n",
    "        if not a.var_names.is_unique: a.var_names_make_unique()\n",
    "        ds = Path(f).name.replace(\"__microglia_scored.h5ad\", \"\")\n",
    "        cohorts[ds] = a\n",
    "        pr.tick(1)\n",
    "        log(pr.line(extra=f\"{ds} | shape={a.n_obs}x{a.n_vars}\"))\n",
    "    hb.stop(\"done\")\n",
    "    return cohorts\n",
    "\n",
    "\n",
    "# PART A: Per-dataset corr + random-effects meta + LODO\n",
    "\n",
    "def random_effects_meta(r_list, n_list):\n",
    "    \"\"\"Random-effects meta with pooled CI + Q-test p-value.\"\"\"\n",
    "    items = [(r, n) for r, n in zip(r_list, n_list)\n",
    "             if pd.notna(r) and pd.notna(n) and float(n) > 3]\n",
    "    out = {\"pooled_r\": np.nan, \"pooled_ci_lo\": np.nan, \"pooled_ci_hi\": np.nan,\n",
    "           \"I2\": np.nan, \"tau2\": np.nan, \"Q\": np.nan, \"Q_p\": np.nan, \"k\": 0}\n",
    "    if len(items) == 0: return out\n",
    "    r = np.array([x[0] for x in items], float)\n",
    "    n = np.array([x[1] for x in items], float)\n",
    "    z = np.arctanh(np.clip(r, -0.999999, 0.999999))\n",
    "    v = 1.0 / (n - 3.0)\n",
    "    w = 1.0 / v\n",
    "    z_fixed = np.sum(w * z) / np.sum(w)\n",
    "    Q = float(np.sum(w * (z - z_fixed) ** 2))\n",
    "    df = max(0, len(z) - 1)\n",
    "    C  = np.sum(w) - (np.sum(w**2) / np.sum(w))\n",
    "    tau2 = max(0.0, (Q - df) / max(C, 1e-12))\n",
    "    w_re = 1.0 / (v + tau2)\n",
    "    z_re = float(np.sum(w_re * z) / np.sum(w_re))\n",
    "    se_z_re = float(1.0 / np.sqrt(np.sum(w_re)))\n",
    "    pooled_r = float(np.tanh(z_re))\n",
    "    ci_lo_z = z_re - 1.96 * se_z_re\n",
    "    ci_hi_z = z_re + 1.96 * se_z_re\n",
    "    I2 = 0.0 if Q <= df else float(max(0.0, (Q - df) / max(Q, 1e-12)) * 100.0)\n",
    "    # Q-test p-value (chi-squared with df degrees of freedom)\n",
    "    Q_p = float(1.0 - scipy_stats.chi2.cdf(Q, df)) if df > 0 else np.nan\n",
    "    out.update({\n",
    "        \"pooled_r\": pooled_r,\n",
    "        \"pooled_ci_lo\": float(np.tanh(ci_lo_z)),\n",
    "        \"pooled_ci_hi\": float(np.tanh(ci_hi_z)),\n",
    "        \"I2\": I2,\n",
    "        \"tau2\": float(tau2),\n",
    "        \"Q\": Q,\n",
    "        \"Q_p\": Q_p,\n",
    "        \"k\": int(len(z)),\n",
    "        \"pooled_z\": z_re,\n",
    "        \"se_z\": se_z_re,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_mes_corr_table(cohorts):\n",
    "    \"\"\"Per-dataset correlations with p-values, CIs, partial correlations.\"\"\"\n",
    "    rows = []\n",
    "    pr = Progress(\"PartA per-dataset corr\", total=len(cohorts))\n",
    "    hb = Heartbeat(\"PartA correlations\", every_s=20)\n",
    "    hb.start(\"start\")\n",
    "    for ds, a in cohorts.items():\n",
    "        donor_col = detect_donor_col(a)\n",
    "        obs = a.obs.copy()\n",
    "        gr_col   = detect_gr_col(obs)\n",
    "        mes_cols = detect_mes_cols(obs)\n",
    "        if gr_col is None or len(mes_cols) == 0:\n",
    "            rows.append({\"dataset\": ds, \"level\": \"NA\", \"N\": np.nan, \"note\": \"missing GR/MES\"})\n",
    "            pr.tick(1); continue\n",
    "\n",
    "        if donor_col is not None:\n",
    "            # Donor-level aggregation\n",
    "            grp   = obs.groupby(obs[donor_col].astype(str))\n",
    "            dmean = grp[[gr_col] + mes_cols].mean(numeric_only=True)\n",
    "            N     = int(dmean.shape[0])\n",
    "            level = \"donor\"\n",
    "            # Build covariates\n",
    "            covars = build_covariates(dmean, obs, donor_col)\n",
    "            for mes in mes_cols:\n",
    "                raw = corr_with_pvalue(dmean[gr_col], dmean[mes], method=\"spearman\")\n",
    "                pcr = partial_corr(dmean, gr_col, mes, covars, method=\"spearman\")\n",
    "                rows.append({\n",
    "                    \"dataset\": ds, \"level\": level, \"N\": N, \"donor_col\": donor_col,\n",
    "                    \"MES\": canonical_mes_label(mes),\n",
    "                    \"r\": raw[\"r\"], \"p\": raw[\"p\"],\n",
    "                    \"ci_lo\": raw[\"ci_lo\"], \"ci_hi\": raw[\"ci_hi\"],\n",
    "                    \"r_partial\": pcr[\"r\"], \"p_partial\": pcr[\"p\"],\n",
    "                    \"covariates\": pcr.get(\"covariates\", \"\"),\n",
    "                    \"GR_col_used\": gr_col, \"MES_col_used\": mes,\n",
    "                })\n",
    "        else:\n",
    "            N     = int(obs.shape[0])\n",
    "            level = \"cell\"\n",
    "            for mes in mes_cols:\n",
    "                raw = corr_with_pvalue(obs[gr_col], obs[mes], method=\"spearman\")\n",
    "                rows.append({\n",
    "                    \"dataset\": ds, \"level\": level, \"N\": N, \"donor_col\": None,\n",
    "                    \"MES\": canonical_mes_label(mes),\n",
    "                    \"r\": raw[\"r\"], \"p\": raw[\"p\"],\n",
    "                    \"ci_lo\": raw[\"ci_lo\"], \"ci_hi\": raw[\"ci_hi\"],\n",
    "                    \"r_partial\": np.nan, \"p_partial\": np.nan,\n",
    "                    \"covariates\": \"\",\n",
    "                    \"GR_col_used\": gr_col, \"MES_col_used\": mes,\n",
    "                })\n",
    "        pr.tick(1)\n",
    "        log(pr.line(extra=f\"done={ds} | level={level} | N={N}\"))\n",
    "    hb.stop(\"done\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    if len(df) and \"p\" in df.columns:\n",
    "        df[\"q_BH\"] = bh_fdr(df[\"p\"].values)\n",
    "        if \"p_partial\" in df.columns:\n",
    "            df[\"q_partial_BH\"] = bh_fdr(df[\"p_partial\"].values)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_forest_plot(meta_df, corr_df, out_png):\n",
    "    \"\"\"Proper forest plot: per-study CIs + pooled diamond.\"\"\"\n",
    "    mes_list = sorted(meta_df[\"MES\"].dropna().unique())\n",
    "    fig, axes = plt.subplots(1, len(mes_list),\n",
    "                             figsize=(3.5 * len(mes_list),\n",
    "                                      max(3.0, 0.5 * len(corr_df[\"dataset\"].unique()))),\n",
    "                             sharey=True, squeeze=False)\n",
    "    axes = axes.ravel()\n",
    "    datasets = sorted(corr_df[\"dataset\"].dropna().unique())\n",
    "    y_pos = {ds: i for i, ds in enumerate(reversed(datasets))}\n",
    "\n",
    "    for ax_i, mes in enumerate(mes_list):\n",
    "        ax = axes[ax_i]\n",
    "        sub_meta = meta_df[meta_df[\"MES\"] == mes].iloc[0] if len(meta_df[meta_df[\"MES\"]==mes]) else None\n",
    "        sub_corr = corr_df[(corr_df[\"MES\"] == mes) & corr_df[\"r\"].notna()]\n",
    "        for _, row in sub_corr.iterrows():\n",
    "            ds = row[\"dataset\"]\n",
    "            if ds not in y_pos: continue\n",
    "            yi = y_pos[ds]\n",
    "            ci_lo = row.get(\"ci_lo\", np.nan)\n",
    "            ci_hi = row.get(\"ci_hi\", np.nan)\n",
    "            xerr_lo = row[\"r\"] - ci_lo if np.isfinite(ci_lo) else 0\n",
    "            xerr_hi = ci_hi - row[\"r\"] if np.isfinite(ci_hi) else 0\n",
    "            color = PAL_DATASETS[ax_i % len(PAL_DATASETS)]\n",
    "            ax.errorbar(row[\"r\"], yi, xerr=[[xerr_lo], [xerr_hi]],\n",
    "                        fmt=\"o\", color=color, ecolor=\"black\",\n",
    "                        elinewidth=0.7, capsize=2, markersize=5)\n",
    "            star = sig_stars(row.get(\"q_BH\", row.get(\"p\")))\n",
    "            if star != \"ns\":\n",
    "                ax.text(ci_hi + 0.02 if np.isfinite(ci_hi) else row[\"r\"] + 0.05,\n",
    "                        yi, star, va=\"center\", fontsize=6, fontweight=\"bold\")\n",
    "        # Pooled diamond\n",
    "        if sub_meta is not None and np.isfinite(sub_meta.get(\"pooled_r\", np.nan)):\n",
    "            pr = sub_meta[\"pooled_r\"]\n",
    "            plo = sub_meta.get(\"pooled_ci_lo\", pr)\n",
    "            phi = sub_meta.get(\"pooled_ci_hi\", pr)\n",
    "            dy = -1.0\n",
    "            diamond_x = [plo, pr, phi, pr, plo]\n",
    "            diamond_y = [dy, dy-0.3, dy, dy+0.3, dy]\n",
    "            ax.fill(diamond_x, diamond_y, color=\"red\", alpha=0.7)\n",
    "            ax.text(phi + 0.02, dy, f\"r={pr:.2f}\", va=\"center\", fontsize=6, color=\"red\")\n",
    "        ax.axvline(0, color=\"grey\", linewidth=0.6, linestyle=\"--\")\n",
    "        ax.set_xlabel(\"Spearman ρ\")\n",
    "        ax.set_title(mes, fontsize=8, fontweight=\"bold\")\n",
    "        if ax_i == 0:\n",
    "            ax.set_yticks(list(y_pos.values()))\n",
    "            ax.set_yticklabels(list(y_pos.keys()))\n",
    "    fig.suptitle(\"Meta-analysis: corr(GR, MES) — per-study CIs + pooled (◆)\",\n",
    "                 fontsize=9, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    save_fig(out_png, fig)\n",
    "\n",
    "\n",
    "def make_heatmap(corr_df, out_png):\n",
    "    \"\"\"Annotated heatmap with significance markers.\"\"\"\n",
    "    df = corr_df[corr_df[\"MES\"].notna()].copy()\n",
    "    piv_r = df.pivot_table(index=\"dataset\", columns=\"MES\", values=\"r\", aggfunc=\"mean\")\n",
    "    piv_q = df.pivot_table(index=\"dataset\", columns=\"MES\", values=\"q_BH\", aggfunc=\"mean\") if \"q_BH\" in df.columns else None\n",
    "    ordered = [f\"MES0{i}\" for i in range(1, 9)]\n",
    "    cols = [c for c in ordered if c in piv_r.columns] + [c for c in piv_r.columns if c not in ordered]\n",
    "    piv_r = piv_r[cols]\n",
    "    if piv_q is not None:\n",
    "        piv_q = piv_q.reindex(columns=cols, index=piv_r.index)\n",
    "    fig, ax = plt.subplots(figsize=(10, max(2.5, 0.5 * piv_r.shape[0])))\n",
    "    if HAS_SNS:\n",
    "        sns.heatmap(piv_r, ax=ax, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0,\n",
    "                    linewidths=0.5, linecolor=\"white\",\n",
    "                    cbar_kws={\"label\": \"Spearman ρ\", \"shrink\": 0.8})\n",
    "        # Significance markers\n",
    "        if piv_q is not None:\n",
    "            for i, ds in enumerate(piv_r.index):\n",
    "                for j, mes in enumerate(piv_r.columns):\n",
    "                    q = piv_q.loc[ds, mes] if ds in piv_q.index and mes in piv_q.columns else np.nan\n",
    "                    s = sig_stars(q)\n",
    "                    if s != \"ns\":\n",
    "                        ax.text(j + 0.5, i + 0.82, s, ha=\"center\", va=\"center\", fontsize=6)\n",
    "    else:\n",
    "        im = ax.imshow(piv_r.values, aspect=\"auto\", cmap=\"RdBu_r\")\n",
    "        ax.set_xticks(np.arange(piv_r.shape[1])); ax.set_xticklabels(piv_r.columns, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(np.arange(piv_r.shape[0])); ax.set_yticklabels(piv_r.index)\n",
    "        plt.colorbar(im, ax=ax, label=\"r\", shrink=0.8)\n",
    "    ax.set_title(\"Per-dataset corr(GR, MES) — donor-level where available\")\n",
    "    save_fig(out_png, fig)\n",
    "\n",
    "\n",
    "def run_partA(cohorts):\n",
    "    log(\"=\" * 50)\n",
    "    log(\"PART A: Correlations + Meta-analysis\")\n",
    "    corr_df = compute_mes_corr_table(cohorts)\n",
    "    save_xlsx(corr_df, TAB_DIR / \"Supplementary_Table5_PerDatasetCorr.xlsx\")\n",
    "\n",
    "    if corr_df.shape[0] == 0 or \"MES\" not in corr_df.columns:\n",
    "        log(\"[WARN] Part A: 0 correlation rows.\")\n",
    "        meta_df = pd.DataFrame()\n",
    "        i2_df   = pd.DataFrame()\n",
    "        lodo_df = pd.DataFrame()\n",
    "        return corr_df, meta_df, i2_df, lodo_df\n",
    "\n",
    "    mes_list = sorted(corr_df[\"MES\"].dropna().unique().tolist())\n",
    "\n",
    "    # Meta-analysis per MES\n",
    "    meta_rows = []\n",
    "    for mes in mes_list:\n",
    "        sub = corr_df[corr_df[\"MES\"] == mes].copy()\n",
    "        out = random_effects_meta(sub[\"r\"].tolist(), sub[\"N\"].tolist())\n",
    "        meta_rows.append({\"MES\": mes, **out})\n",
    "    meta_df = pd.DataFrame(meta_rows)\n",
    "\n",
    "    # Figures\n",
    "    if meta_df.shape[0] > 0 and corr_df.shape[0] > 0:\n",
    "        make_forest_plot(meta_df, corr_df, FIG_DIR / \"Main_Fig5A_Forest_Meta.png\")\n",
    "    if corr_df.shape[0] > 0:\n",
    "        make_heatmap(corr_df, FIG_DIR / \"Main_Fig5B_PooledHeatmap.png\")\n",
    "\n",
    "    # I2 table + figure\n",
    "    i2_df = meta_df[[\"MES\", \"I2\", \"tau2\", \"Q\", \"Q_p\", \"k\"]].copy() if len(meta_df) else pd.DataFrame()\n",
    "    save_xlsx(meta_df, TAB_DIR / \"Main_Table5.xlsx\")\n",
    "    save_xlsx(i2_df,   TAB_DIR / \"Supplementary_Table5_I2.xlsx\")\n",
    "\n",
    "    if len(i2_df) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(6, 3))\n",
    "        colors = [\"#D6604D\" if i2 > 50 else \"#FDAE6B\" if i2 > 25 else \"#66C2A5\"\n",
    "                  for i2 in i2_df[\"I2\"].values]\n",
    "        ax.bar(i2_df[\"MES\"], i2_df[\"I2\"], color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "        for i, (_, row) in enumerate(i2_df.iterrows()):\n",
    "            s = sig_stars(row.get(\"Q_p\"))\n",
    "            if s != \"ns\":\n",
    "                ax.text(i, row[\"I2\"] + 1, s, ha=\"center\", fontsize=7, fontweight=\"bold\")\n",
    "        ax.axhline(25, color=\"grey\", linewidth=0.5, linestyle=\":\")\n",
    "        ax.axhline(50, color=\"grey\", linewidth=0.5, linestyle=\":\")\n",
    "        ax.axhline(75, color=\"grey\", linewidth=0.5, linestyle=\":\")\n",
    "        ax.set_ylabel(\"I² (%)\")\n",
    "        ax.set_title(\"Heterogeneity by MES (Q-test significance annotated)\")\n",
    "        ax.set_xticklabels(i2_df[\"MES\"], rotation=45, ha=\"right\")\n",
    "        if HAS_SNS: sns.despine(ax=ax)\n",
    "        save_fig(FIG_DIR / \"Supplementary_Fig5B_I2.png\", fig)\n",
    "\n",
    "    # LODO\n",
    "    lodo = []\n",
    "    for mes in mes_list:\n",
    "        sub = corr_df[corr_df[\"MES\"] == mes].copy()\n",
    "        datasets = sorted(sub[\"dataset\"].dropna().unique().tolist())\n",
    "        for leave in datasets:\n",
    "            sub2 = sub[sub[\"dataset\"] != leave]\n",
    "            out = random_effects_meta(sub2[\"r\"].tolist(), sub2[\"N\"].tolist())\n",
    "            lodo.append({\"MES\": mes, \"leave_out\": leave,\n",
    "                         \"pooled_r\": out[\"pooled_r\"],\n",
    "                         \"pooled_ci_lo\": out[\"pooled_ci_lo\"],\n",
    "                         \"pooled_ci_hi\": out[\"pooled_ci_hi\"],\n",
    "                         \"I2\": out[\"I2\"], \"k\": out[\"k\"]})\n",
    "    lodo_df = pd.DataFrame(lodo)\n",
    "    save_xlsx(lodo_df, TAB_DIR / \"Supplementary_Table5_LODO.xlsx\")\n",
    "\n",
    "    if len(lodo_df) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "        for mes in mes_list:\n",
    "            sub = lodo_df[lodo_df[\"MES\"] == mes]\n",
    "            if sub.empty: continue\n",
    "            ax.errorbar(sub[\"leave_out\"], sub[\"pooled_r\"],\n",
    "                        yerr=[sub[\"pooled_r\"] - sub[\"pooled_ci_lo\"],\n",
    "                              sub[\"pooled_ci_hi\"] - sub[\"pooled_r\"]],\n",
    "                        fmt=\"o-\", label=mes, markersize=4, linewidth=0.8,\n",
    "                        capsize=2, alpha=0.8)\n",
    "        ax.axhline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\n",
    "        ax.set_ylabel(\"LODO pooled ρ (95% CI)\")\n",
    "        ax.set_title(\"Leave-one-dataset-out sensitivity\")\n",
    "        ax.legend(fontsize=6, ncol=4, loc=\"upper right\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        if HAS_SNS: sns.despine(ax=ax)\n",
    "        save_fig(FIG_DIR / \"Supplementary_Fig5A_LODO.png\", fig)\n",
    "\n",
    "    return corr_df, meta_df, i2_df, lodo_df\n",
    "\n",
    "\n",
    "# PART B: Robustness (raw vs adjusted; donor vs cell)\n",
    "\n",
    "def run_partB(corr_df, cohorts):\n",
    "    log(\"=\" * 50)\n",
    "    log(\"PART B: Robustness\")\n",
    "\n",
    "    # Step 2: recompute cell-level for donor datasets\n",
    "    hb2 = Heartbeat(\"PartB cell recompute\", every_s=30)\n",
    "    hb2.start(\"start\")\n",
    "    donor_datasets = [(ds, detect_donor_col(a)) for ds, a in cohorts.items() if detect_donor_col(a)]\n",
    "    pr = Progress(\"PartB cell recompute\", total=len(donor_datasets))\n",
    "    out_rows = []\n",
    "    for ds, donor_col in donor_datasets:\n",
    "        a = cohorts[ds]; obs = a.obs\n",
    "        gr_col   = detect_gr_col(obs);  mes_cols = detect_mes_cols(obs)\n",
    "        if gr_col is None or not mes_cols: pr.tick(1); continue\n",
    "        pos = stratified_subsample_by_donor_positions(obs, donor_col, MAX_CELLS_FOR_CELL_RECOMPUTE, 42)\n",
    "        aa = a[pos, :].copy()\n",
    "        for mes in mes_cols:\n",
    "            raw = corr_with_pvalue(aa.obs[gr_col], aa.obs[mes], method=\"spearman\")\n",
    "            out_rows.append({\n",
    "                \"dataset\": ds, \"donor_col\": donor_col,\n",
    "                \"N_used\": int(aa.n_obs),\n",
    "                \"donors\": int(pd.Series(aa.obs[donor_col].astype(str)).nunique()),\n",
    "                \"MES\": canonical_mes_label(mes),\n",
    "                \"r_cell_recompute\": raw[\"r\"], \"p_cell\": raw[\"p\"],\n",
    "                \"ci_lo_cell\": raw[\"ci_lo\"], \"ci_hi_cell\": raw[\"ci_hi\"],\n",
    "            })\n",
    "        pr.tick(1)\n",
    "        log(pr.line(extra=f\"done={ds}\"))\n",
    "        del aa; gc.collect()\n",
    "    hb2.stop(\"done\")\n",
    "    df_cell_recompute = pd.DataFrame(out_rows)\n",
    "    save_xlsx(df_cell_recompute, TAB_DIR / \"Supplementary_Table5_DonorDatasets_CellRecompute.xlsx\")\n",
    "\n",
    "    # Step 1: raw vs adjusted figure\n",
    "    hb = Heartbeat(\"PartB figs\", every_s=30)\n",
    "    hb.start(\"start\")\n",
    "    df = corr_df.copy()\n",
    "    if df.shape[0] == 0 or \"MES\" not in df.columns:\n",
    "        save_xlsx(pd.DataFrame(), TAB_DIR / \"Supplementary_Table5_RawVsAdjusted.xlsx\")\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        ax.text(0.05, 0.5, \"No data\", fontsize=10); ax.axis(\"off\")\n",
    "        save_fig(FIG_DIR / \"Main_Fig5C_RawVsAdjusted.png\", fig)\n",
    "    else:\n",
    "        df = df[df[\"MES\"].notna()]\n",
    "        piv = df.pivot_table(index=[\"dataset\", \"MES\"], columns=\"level\", values=\"r\", aggfunc=\"mean\").reset_index()\n",
    "        if \"cell\"  not in piv.columns: piv[\"cell\"]  = np.nan\n",
    "        if \"donor\" not in piv.columns: piv[\"donor\"] = np.nan\n",
    "        piv[\"adjusted\"] = piv[\"donor\"].where(piv[\"donor\"].notna(), piv[\"cell\"])\n",
    "        # Also include partial r where available\n",
    "        piv_partial = df.pivot_table(index=[\"dataset\",\"MES\"], values=\"r_partial\", aggfunc=\"mean\").reset_index()\n",
    "        if len(piv_partial):\n",
    "            piv = piv.merge(piv_partial, on=[\"dataset\",\"MES\"], how=\"left\")\n",
    "        summ2 = piv.groupby(\"dataset\")[[\"cell\",\"adjusted\"]].mean(numeric_only=True).reset_index()\n",
    "        if \"r_partial\" in piv.columns:\n",
    "            summ_partial = piv.groupby(\"dataset\")[\"r_partial\"].mean().reset_index()\n",
    "            summ2 = summ2.merge(summ_partial, on=\"dataset\", how=\"left\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6.5, max(2.5, 0.45 * summ2.shape[0])))\n",
    "        y = np.arange(summ2.shape[0])[::-1]\n",
    "        ax.scatter(summ2[\"cell\"],     y, label=\"raw (cell)\",     marker=\"s\", s=40, zorder=3)\n",
    "        ax.scatter(summ2[\"adjusted\"], y, label=\"adjusted (donor)\", marker=\"o\", s=50, zorder=3)\n",
    "        if \"r_partial\" in summ2.columns:\n",
    "            ax.scatter(summ2[\"r_partial\"], y + 0.15, label=\"partial (covar-adj)\",\n",
    "                       marker=\"D\", s=35, color=\"#66C2A5\", zorder=3)\n",
    "        ax.set_yticks(y); ax.set_yticklabels(summ2[\"dataset\"])\n",
    "        ax.axvline(0, linewidth=0.8, color=\"grey\", linestyle=\"--\")\n",
    "        ax.set_xlabel(\"Mean ρ across MES\")\n",
    "        ax.set_title(\"Robustness: raw vs donor-adjusted vs covariate-adjusted\")\n",
    "        ax.legend(fontsize=7)\n",
    "        if HAS_SNS: sns.despine(ax=ax)\n",
    "        save_fig(FIG_DIR / \"Main_Fig5C_RawVsAdjusted.png\", fig)\n",
    "        save_xlsx(summ2, TAB_DIR / \"Supplementary_Table5_RawVsAdjusted.xlsx\")\n",
    "\n",
    "    # Donor vs cell overlap\n",
    "    overlap = pd.DataFrame()\n",
    "    if df.shape[0] > 0 and df_cell_recompute.shape[0] > 0:\n",
    "        donor_only = df[df[\"level\"]==\"donor\"][[\"dataset\",\"MES\",\"r\",\"ci_lo\",\"ci_hi\"]].rename(\n",
    "            columns={\"r\": \"r_donor\", \"ci_lo\": \"ci_lo_donor\", \"ci_hi\": \"ci_hi_donor\"})\n",
    "        cell_only  = df_cell_recompute[[\"dataset\",\"MES\",\"r_cell_recompute\",\"ci_lo_cell\",\"ci_hi_cell\"]].rename(\n",
    "            columns={\"r_cell_recompute\": \"r_cell\"})\n",
    "        overlap = donor_only.merge(cell_only, on=[\"dataset\",\"MES\"], how=\"inner\")\n",
    "    save_xlsx(overlap, TAB_DIR / \"Supplementary_Table5_DonorVsCell_OverlapUsedForFig.xlsx\")\n",
    "\n",
    "    if overlap.shape[0] > 0:\n",
    "        fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "        ax.scatter(overlap[\"r_cell\"], overlap[\"r_donor\"], s=30, alpha=0.7,\n",
    "                   color=\"#4393C3\", edgecolors=\"black\", linewidth=0.3)\n",
    "        lo = float(np.nanmin([overlap[\"r_cell\"].min(), overlap[\"r_donor\"].min()])) - 0.1\n",
    "        hi = float(np.nanmax([overlap[\"r_cell\"].max(), overlap[\"r_donor\"].max()])) + 0.1\n",
    "        ax.plot([lo, hi], [lo, hi], color=\"grey\", linewidth=0.8, linestyle=\"--\")\n",
    "        ax.axhline(0, linewidth=0.5, color=\"grey\"); ax.axvline(0, linewidth=0.5, color=\"grey\")\n",
    "        # Correlation between donor and cell estimates\n",
    "        cc = corr_with_pvalue(overlap[\"r_cell\"], overlap[\"r_donor\"], method=\"pearson\")\n",
    "        ax.set_title(f\"Donor vs Cell ρ | concordance r={cc['r']:.2f}, p={cc['p']:.2e}\")\n",
    "        ax.set_xlabel(\"Cell-level ρ (recomputed)\")\n",
    "        ax.set_ylabel(\"Donor-level ρ (Part A)\")\n",
    "        if HAS_SNS: sns.despine(ax=ax)\n",
    "        save_fig(FIG_DIR / \"Supplementary_Fig5C_DonorVsCell.png\", fig)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        ax.text(0.05, 0.5, \"No donor+cell overlap\", fontsize=10); ax.axis(\"off\")\n",
    "        save_fig(FIG_DIR / \"Supplementary_Fig5C_DonorVsCell.png\", fig)\n",
    "    hb.stop(\"done\")\n",
    "    return df_cell_recompute, overlap\n",
    "\n",
    "\n",
    "# PART C: GR stratified DE + volcano + cross-dataset overlap\n",
    "\n",
    "def pseudobulk_by_donor(adata, donor_col, layer_counts=\"counts\",\n",
    "                        use_log_norm=True, progress_label=None):\n",
    "    if donor_col not in adata.obs.columns: return None\n",
    "    donors = adata.obs[donor_col].astype(str)\n",
    "    uniq   = donors.unique().tolist()\n",
    "    if len(uniq) < 4: return None\n",
    "    Xsrc = adata.layers[layer_counts] if layer_counts in adata.layers else adata.X\n",
    "    rows = []\n",
    "    for i, d in enumerate(uniq, 1):\n",
    "        pos = np.flatnonzero(donors.values == d)\n",
    "        if pos.size == 0: continue\n",
    "        v = np.asarray(Xsrc[pos, :].sum(axis=0)).ravel()\n",
    "        rows.append(v)\n",
    "        if i == 1 or i == len(uniq) or i % max(1, len(uniq)//4) == 0:\n",
    "            elapsed = time.time()\n",
    "            log(f\"PROG PartC pseudobulk: {i}/{len(uniq)} | {progress_label or ''}\")\n",
    "    if not rows: return None\n",
    "    Xpb = np.vstack(rows)\n",
    "    pb = ad.AnnData(X=Xpb, obs=pd.DataFrame({donor_col: uniq}), var=adata.var.copy())\n",
    "    pb.obs_names = pd.Index(uniq)\n",
    "    gr_col = detect_gr_col(adata.obs)\n",
    "    if gr_col:\n",
    "        pb.obs[gr_col] = adata.obs.groupby(donors)[gr_col].mean(numeric_only=True).reindex(uniq).values\n",
    "    if use_log_norm:\n",
    "        sc.pp.normalize_total(pb, target_sum=1e6)\n",
    "        sc.pp.log1p(pb)\n",
    "    return pb\n",
    "\n",
    "\n",
    "def maybe_run_enrichment(df_de, label):\n",
    "    if not DO_ENRICHMENT or df_de is None or df_de.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        import gseapy as gp\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    sub = df_de.sort_values(\"padj\").head(300)\n",
    "    genes = sub[\"gene\"].astype(str).tolist()\n",
    "    if len(genes) < 20: return pd.DataFrame()\n",
    "    try:\n",
    "        enr = gp.enrichr(gene_list=genes, gene_sets=[\"GO_Biological_Process_2021\"],\n",
    "                         organism=\"Human\", outdir=None, no_plot=True)\n",
    "        res = enr.results.copy()\n",
    "        res.insert(0, \"label\", label)\n",
    "        return res\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def make_volcano(df_de, ds_name, out_png):\n",
    "    \"\"\"Volcano plot with significance thresholds and non-overlapping labels.\n",
    "\n",
    "    FIX: removed bbox from gene labels so adjustText measures true text\n",
    "    extent and arrows anchor from edges not midpoints. Arrows are now\n",
    "    light grey with a simple thin connector style.\n",
    "    \"\"\"\n",
    "    df = df_de.copy()\n",
    "    df[\"neg_log10_padj\"] = -np.log10(df[\"padj\"].clip(lower=1e-300))\n",
    "    df[\"significant\"] = (df[\"padj\"] < 0.05) & (df[\"logFC\"].abs() > 0.25)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 4.5))\n",
    "\n",
    "    # Non-significant\n",
    "    ns = df[~df[\"significant\"]]\n",
    "    ax.scatter(ns[\"logFC\"], ns[\"neg_log10_padj\"], s=5, alpha=0.3, color=\"#CCCCCC\", label=\"ns\")\n",
    "\n",
    "    # Up\n",
    "    up = df[df[\"significant\"] & (df[\"logFC\"] > 0)]\n",
    "    ax.scatter(up[\"logFC\"], up[\"neg_log10_padj\"], s=8, alpha=0.6, color=\"#D6604D\", label=f\"Up ({len(up)})\")\n",
    "\n",
    "    # Down\n",
    "    dn = df[df[\"significant\"] & (df[\"logFC\"] < 0)]\n",
    "    ax.scatter(dn[\"logFC\"], dn[\"neg_log10_padj\"], s=8, alpha=0.6, color=\"#4393C3\", label=f\"Down ({len(dn)})\")\n",
    "\n",
    "    ax.axhline(-np.log10(0.05), color=\"grey\", linewidth=0.5, linestyle=\"--\")\n",
    "    ax.axvline(0.25,  color=\"grey\", linewidth=0.5, linestyle=\":\")\n",
    "    ax.axvline(-0.25, color=\"grey\", linewidth=0.5, linestyle=\":\")\n",
    "\n",
    "    # Label top genes — NO bbox, light grey arrows, simple connector\n",
    "    top = df.nlargest(10, \"neg_log10_padj\")\n",
    "    texts = []\n",
    "    for _, row in top.iterrows():\n",
    "        texts.append(ax.text(row[\"logFC\"], row[\"neg_log10_padj\"],\n",
    "                             row[\"gene\"], fontsize=5.5, fontweight=\"bold\"))\n",
    "    adjust_text(texts, ax=ax,\n",
    "                force_points=(0.6, 0.9),\n",
    "                force_text=(0.6, 0.9),\n",
    "                expand_points=(1.5, 1.5),\n",
    "                expand_text=(1.2, 1.2),\n",
    "                arrowprops=dict(arrowstyle=\"-\",\n",
    "                                color=\"#999999\",\n",
    "                                lw=0.5))\n",
    "\n",
    "    ax.set_xlabel(\"log₂FC (GR_high vs GR_low)\")\n",
    "    ax.set_ylabel(\"-log₁₀(padj)\")\n",
    "    ax.set_title(f\"{ds_name}: GR-stratified DE\")\n",
    "    ax.legend(fontsize=6, loc=\"upper left\")\n",
    "    if HAS_SNS: sns.despine(ax=ax)\n",
    "    save_fig(out_png, fig)\n",
    "\n",
    "\n",
    "def compute_de_overlap(de_tables):\n",
    "    \"\"\"Cross-dataset gene overlap analysis.\"\"\"\n",
    "    if len(de_tables) < 2: return pd.DataFrame()\n",
    "    sig_genes = {}\n",
    "    for ds, df in de_tables.items():\n",
    "        sig = df[(df[\"padj\"] < 0.05) & (df[\"logFC\"].abs() > 0.25)][\"gene\"].tolist()\n",
    "        if sig: sig_genes[ds] = set(sig)\n",
    "    if len(sig_genes) < 2: return pd.DataFrame()\n",
    "    rows = []\n",
    "    # Pairwise overlaps\n",
    "    for (ds1, g1), (ds2, g2) in combinations(sig_genes.items(), 2):\n",
    "        overlap = g1 & g2\n",
    "        union   = g1 | g2\n",
    "        jaccard = len(overlap) / len(union) if union else 0\n",
    "        rows.append({\n",
    "            \"ds1\": ds1, \"ds2\": ds2,\n",
    "            \"n_sig_ds1\": len(g1), \"n_sig_ds2\": len(g2),\n",
    "            \"n_overlap\": len(overlap), \"n_union\": len(union),\n",
    "            \"jaccard\": jaccard,\n",
    "            \"overlap_genes\": \",\".join(sorted(overlap)[:50])\n",
    "        })\n",
    "    # Genes significant in >=2 datasets\n",
    "    from collections import Counter\n",
    "    gene_counts = Counter()\n",
    "    for gs in sig_genes.values():\n",
    "        gene_counts.update(gs)\n",
    "    replicated = {g: c for g, c in gene_counts.items() if c >= 2}\n",
    "    for g, c in sorted(replicated.items(), key=lambda x: -x[1])[:50]:\n",
    "        rows.append({\n",
    "            \"ds1\": \"REPLICATED\", \"ds2\": f\"in_{c}_datasets\",\n",
    "            \"n_overlap\": c, \"overlap_genes\": g\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_partC(cohorts):\n",
    "    log(\"=\" * 50)\n",
    "    log(\"PART C: GR-stratified DE + Volcano + Overlap\")\n",
    "    hb = Heartbeat(\"PartC DE\", every_s=30)\n",
    "    hb.start(\"start\")\n",
    "    pr_ds = Progress(\"PartC datasets\", total=len(cohorts))\n",
    "    de_tables  = {}\n",
    "    eff_rows   = []\n",
    "    enrich_rows = []\n",
    "\n",
    "    for ds, a in cohorts.items():\n",
    "        hb.update(f\"starting {ds}\")\n",
    "        log(f\"DE start: {ds} (cells={a.n_obs})\")\n",
    "        donor_col = detect_donor_col(a)\n",
    "        gr_col    = detect_gr_col(a.obs)\n",
    "        if gr_col is None:\n",
    "            log(f\"[WARN] {ds}: no GR column; skipping.\")\n",
    "            pr_ds.tick(1); continue\n",
    "        use_pseudobulk = donor_col is not None\n",
    "        chosen_level   = None\n",
    "\n",
    "        # Pseudobulk path\n",
    "        if use_pseudobulk:\n",
    "            pb = pseudobulk_by_donor(a, donor_col=donor_col, progress_label=ds)\n",
    "            if pb is None:\n",
    "                use_pseudobulk = False\n",
    "            else:\n",
    "                add_gr_group(pb, gr_col=gr_col, out_col=\"GR_group\")\n",
    "                df_de = de_rank_genes_groups(pb, \"GR_group\", \"GR_high\", \"GR_low\",\n",
    "                                             method=\"wilcoxon\", min_n_per_group=3)\n",
    "                if df_de is not None and df_de.shape[0] > 0:\n",
    "                    de_tables[ds] = df_de\n",
    "                    chosen_level = \"donor_pseudobulk\"\n",
    "                    # Volcano plot\n",
    "                    make_volcano(df_de, ds, FIG_DIR / f\"Supplementary_Fig5_Volcano_{ds}.png\")\n",
    "                    top = df_de.sort_values(\"padj\").head(50).copy()\n",
    "                    top[\"dataset\"] = ds; top[\"level\"] = chosen_level\n",
    "                    eff_rows.append(top)\n",
    "                    enr = maybe_run_enrichment(df_de, f\"{ds}|{chosen_level}\")\n",
    "                    if enr is not None and enr.shape[0] > 0: enrich_rows.append(enr)\n",
    "                    log(f\"DE end: {ds} (level={chosen_level}, DE_rows={df_de.shape[0]})\")\n",
    "                else:\n",
    "                    use_pseudobulk = False\n",
    "\n",
    "        # Cell fallback\n",
    "        if not use_pseudobulk:\n",
    "            if donor_col and donor_col in a.obs.columns:\n",
    "                pos = stratified_subsample_by_donor_positions(a.obs, donor_col, MAX_CELLS_FOR_CELL_DE, 42)\n",
    "            else:\n",
    "                if a.n_obs > MAX_CELLS_FOR_CELL_DE:\n",
    "                    rng = np.random.default_rng(42)\n",
    "                    pos = rng.choice(np.arange(a.n_obs, dtype=int), size=MAX_CELLS_FOR_CELL_DE, replace=False)\n",
    "                else:\n",
    "                    pos = np.arange(a.n_obs, dtype=int)\n",
    "            aa = a[pos, :].copy()\n",
    "            add_gr_group(aa, gr_col=gr_col, out_col=\"GR_group\")\n",
    "            df_de = de_rank_genes_groups(aa, \"GR_group\", \"GR_high\", \"GR_low\",\n",
    "                                          method=\"wilcoxon\", min_n_per_group=10)\n",
    "            if df_de is not None and df_de.shape[0] > 0:\n",
    "                de_tables[ds] = df_de\n",
    "                chosen_level = \"cell_level_subsampled\"\n",
    "                make_volcano(df_de, ds, FIG_DIR / f\"Supplementary_Fig5_Volcano_{ds}.png\")\n",
    "                top = df_de.sort_values(\"padj\").head(50).copy()\n",
    "                top[\"dataset\"] = ds; top[\"level\"] = chosen_level\n",
    "                eff_rows.append(top)\n",
    "                enr = maybe_run_enrichment(df_de, f\"{ds}|{chosen_level}\")\n",
    "                if enr is not None and enr.shape[0] > 0: enrich_rows.append(enr)\n",
    "            else:\n",
    "                chosen_level = \"empty\"\n",
    "            del aa; gc.collect()\n",
    "        pr_ds.tick(1)\n",
    "        log(pr_ds.line(extra=f\"done {ds} | level={chosen_level}\"))\n",
    "    hb.stop(\"done\")\n",
    "\n",
    "    df_gr_eff  = pd.concat(eff_rows, ignore_index=True) if eff_rows else pd.DataFrame()\n",
    "    df_enrich  = pd.concat(enrich_rows, ignore_index=True) if enrich_rows else pd.DataFrame()\n",
    "\n",
    "    # Cross-dataset overlap\n",
    "    df_de_overlap = compute_de_overlap(de_tables)\n",
    "    if len(df_de_overlap):\n",
    "        log(f\"DE overlap: {len(df_de_overlap)} rows\")\n",
    "    return de_tables, df_enrich, pd.DataFrame(), df_gr_eff, df_de_overlap\n",
    "\n",
    "\n",
    "# Supplementary Table 5 index\n",
    "\n",
    "def write_table5_index():\n",
    "    rows = [\n",
    "        {\"artifact\": \"Main_Table5.xlsx\",\n",
    "         \"role\": \"Pooled random-effects meta: pooled_r, 95% CI, I², tau², Q, Q_p per MES.\",\n",
    "         \"produced_by\": \"Part A\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_PerDatasetCorr.xlsx\",\n",
    "         \"role\": \"Per-dataset ρ(GR, MES) with p-values, bootstrap CIs, partial ρ, BH-FDR q.\",\n",
    "         \"produced_by\": \"Part A\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_I2.xlsx\",\n",
    "         \"role\": \"Heterogeneity: I², tau², Q, Q_p per MES.\",\n",
    "         \"produced_by\": \"Part A\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_LODO.xlsx\",\n",
    "         \"role\": \"LODO sensitivity with pooled CIs.\",\n",
    "         \"produced_by\": \"Part A\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_RawVsAdjusted.xlsx\",\n",
    "         \"role\": \"Robustness: raw cell vs donor vs covariate-adjusted mean ρ.\",\n",
    "         \"produced_by\": \"Part B\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_DonorDatasets_CellRecompute.xlsx\",\n",
    "         \"role\": \"Recomputed cell-level ρ for donor datasets with p-values + CIs.\",\n",
    "         \"produced_by\": \"Part B\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_DonorVsCell_OverlapUsedForFig.xlsx\",\n",
    "         \"role\": \"Donor vs cell overlap with CIs.\",\n",
    "         \"produced_by\": \"Part B\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_GR_DE_AllDatasets.xlsx\",\n",
    "         \"role\": \"Full DE results (GR_high vs GR_low) per dataset.\",\n",
    "         \"produced_by\": \"Part C\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_GR_DE_Top50_PerDataset.xlsx\",\n",
    "         \"role\": \"Top 50 DE genes per dataset.\",\n",
    "         \"produced_by\": \"Part C\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_GR_DE_Overlap.xlsx\",\n",
    "         \"role\": \"Cross-dataset DE gene overlap + Jaccard + replicated genes.\",\n",
    "         \"produced_by\": \"Part C\"},\n",
    "        {\"artifact\": \"Supplementary_Table5_GR_Enrichment.xlsx\",\n",
    "         \"role\": \"Enrichment results (if DO_ENRICHMENT=True).\",\n",
    "         \"produced_by\": \"Part C\"},\n",
    "        {\"artifact\": \"NB5_DIAG.xlsx\",\n",
    "         \"role\": \"Diagnostics.\",\n",
    "         \"produced_by\": \"Diagnostics\"},\n",
    "    ]\n",
    "    save_xlsx(pd.DataFrame(rows), TAB_DIR / \"Supplementary_Table5_Index.xlsx\")\n",
    "\n",
    "\n",
    "# MAIN RUNNER\n",
    "\n",
    "log(\"=\" * 60)\n",
    "log(\"NB5 START\")\n",
    "log(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    diag_df = diagnostics()\n",
    "    for _, r in diag_df.iterrows():\n",
    "        if r[\"check\"] in (\"scored matches\", \"detected GR col\", \"detected MES cols\",\n",
    "                          \"detected donor col\", \"detected sex col\", \"detected PMI col\"):\n",
    "            log(f\"[DIAG] {r['check']}: {r['value']}\")\n",
    "except Exception as e:\n",
    "    log(f\"[WARN] diagnostics failed: {repr(e)}\")\n",
    "\n",
    "if RUN_DIAGNOSTICS_ONLY:\n",
    "    log(\"RUN_DIAGNOSTICS_ONLY=True -> stopping.\")\n",
    "else:\n",
    "    tables_nb4 = load_nb4_tables()\n",
    "    cohorts    = load_scored_cohorts()\n",
    "\n",
    "    # --- Part A ---\n",
    "    log(f\"START Part A | RSS={_rss_gb():.2f} GB\")\n",
    "    corr_df, meta_df, i2_df, lodo_df = run_partA(cohorts)\n",
    "    log(f\"END Part A | RSS={_rss_gb():.2f} GB\")\n",
    "\n",
    "    # --- Part B ---\n",
    "    log(f\"START Part B | RSS={_rss_gb():.2f} GB\")\n",
    "    df_cell_recompute, df_overlap_B = run_partB(corr_df, cohorts)\n",
    "    log(f\"END Part B | RSS={_rss_gb():.2f} GB\")\n",
    "\n",
    "    # --- Part C ---\n",
    "    log(f\"START Part C | RSS={_rss_gb():.2f} GB\")\n",
    "    de_tables, df_enrich, df_path_meta, df_gr_eff, df_de_overlap = run_partC(cohorts)\n",
    "    log(f\"END Part C | RSS={_rss_gb():.2f} GB\")\n",
    "\n",
    "    # Save Part C outputs (SAME names)\n",
    "    if len(de_tables) > 0:\n",
    "        all_de = []\n",
    "        for ds, df in de_tables.items():\n",
    "            tmp = df.copy(); tmp.insert(0, \"dataset\", ds); all_de.append(tmp)\n",
    "        save_xlsx(pd.concat(all_de, ignore_index=True),\n",
    "                  TAB_DIR / \"Supplementary_Table5_GR_DE_AllDatasets.xlsx\")\n",
    "    else:\n",
    "        save_xlsx(pd.DataFrame(), TAB_DIR / \"Supplementary_Table5_GR_DE_AllDatasets.xlsx\")\n",
    "\n",
    "    if df_gr_eff is not None and df_gr_eff.shape[0] > 0:\n",
    "        save_xlsx(df_gr_eff, TAB_DIR / \"Supplementary_Table5_GR_DE_Top50_PerDataset.xlsx\")\n",
    "    else:\n",
    "        save_xlsx(pd.DataFrame(), TAB_DIR / \"Supplementary_Table5_GR_DE_Top50_PerDataset.xlsx\")\n",
    "\n",
    "    if df_de_overlap is not None and df_de_overlap.shape[0] > 0:\n",
    "        save_xlsx(df_de_overlap, TAB_DIR / \"Supplementary_Table5_GR_DE_Overlap.xlsx\")\n",
    "\n",
    "    if df_enrich is not None and df_enrich.shape[0] > 0:\n",
    "        save_xlsx(df_enrich, TAB_DIR / \"Supplementary_Table5_GR_Enrichment.xlsx\")\n",
    "\n",
    "    write_table5_index()\n",
    "\n",
    "    log(\"=\" * 60)\n",
    "    log(\"NB5 COMPLETE\")\n",
    "    log(\"=\" * 60)"
   ]
  }
 ]
}