{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB2 — Aim 1: Thymus Profiling, Negative Control & Innate Memory QC\n",
    "\n",
    "Single-cell QC and preprocessing of three thymus cohorts (Tabula Sapiens, Lavaert GSE144870, Le GSE139042), peripheral blood negative control, and innate immune memory datasets.\n",
    "\n",
    "**Paper:** Zafar SA, Qin W. *Thymus-Derived Myeloid Education Signatures Predict Microglial Tolerance Positioning and Are Modulated by Glucocorticoid Stress-Axis Activity.* Neuroimmunomodulation (2026).\n",
    "\n",
    "> **Note:** Update the path variables in section 0 to match your local directory structure before running. Raw data can be obtained from the public repositories listed in Supplementary Table S1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, json, gzip\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import scipy.sparse as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sc.settings.verbosity = 2\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# 0) Paths\n",
    "\n",
    "BASE_DIR = Path(\".\")  # <-- SET TO YOUR PROJECT ROOT\n",
    "RAW_DIR  = BASE_DIR / \"Raw Data\"\n",
    "PROC_DIR = BASE_DIR / \"Process Data\"\n",
    "\n",
    "MANUSCRIPT_DIR = BASE_DIR / \"outputs\" / \"manuscript\"\n",
    "FIG_DIR = MANUSCRIPT_DIR / \"Figures\"\n",
    "TAB_DIR = MANUSCRIPT_DIR / \"Tables\"\n",
    "\n",
    "OUT_AIM1     = PROC_DIR / \"aim1_thymus\"\n",
    "OUT_NEGCTRL  = PROC_DIR / \"negctrl\"\n",
    "OUT_IMM      = PROC_DIR / \"innate_memory\"\n",
    "\n",
    "for d in [OUT_AIM1, OUT_NEGCTRL, OUT_IMM, FIG_DIR, TAB_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert RAW_DIR.exists(), f\"RAW_DIR not found: {RAW_DIR}\"\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"RAW_DIR :\", RAW_DIR)\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n",
    "print(\"MANUSCRIPT_DIR:\", MANUSCRIPT_DIR)\n",
    "print(\"FIG_DIR :\", FIG_DIR)\n",
    "print(\"TAB_DIR :\", TAB_DIR)\n",
    "\n",
    "\n",
    "# 1) Figure style\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": 8,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"axes.titlesize\": 9,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"figure.titlesize\": 9,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "})\n",
    "FIG_DPI = 1200\n",
    "\n",
    "PALETTE_STAGE = [\"#3182bd\", \"#e6550d\"]  # pre / post\n",
    "PALETTE_PASS  = [\"#2ca25f\", \"#de2d26\"]  # pass / fail\n",
    "\n",
    "def save_fig(fig, fname: str, kind: str = \"Supplementary\"):\n",
    "    assert kind in (\"Main\", \"Supplementary\")\n",
    "    out = FIG_DIR / f\"{kind}_{fname}.png\"\n",
    "    fig.savefig(out, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[SAVED FIG] {out}\")\n",
    "\n",
    "def save_table_xlsx(sheets: dict, fname: str, kind: str = \"Supplementary\"):\n",
    "    assert kind in (\"Main\", \"Supplementary\")\n",
    "    out = TAB_DIR / f\"{kind}_{fname}.xlsx\"\n",
    "    with pd.ExcelWriter(out, engine=\"openpyxl\") as w:\n",
    "        for sn, df in sheets.items():\n",
    "            df.to_excel(w, index=False, sheet_name=str(sn)[:31])\n",
    "    print(f\"[SAVED TABLE] {out}\")\n",
    "\n",
    "\n",
    "# 2) Registry\n",
    "\n",
    "REG = {\n",
    "    \"TS_Thymus_filtered\":  RAW_DIR / \"Thymus\" / \"TS_Thymus_filtered.h5ad\",\n",
    "    \"GSE144870_Lavaert\":   RAW_DIR / \"Thymus\" / \"GSE144870\",\n",
    "    \"GSE139042_Le\":        RAW_DIR / \"Thymus\" / \"GSE139042\",\n",
    "    \"GSE133341_Zeng\":      RAW_DIR / \"Thymus\" / \"GSE133341\",\n",
    "    \"TS_Blood_NegCtrl\":    RAW_DIR / \"Peripheral_Myeloid_NegCtrl\" / \"Tabula_Sapiens_Blood.h5ad\",\n",
    "    \"GSE229940_dir\":       RAW_DIR / \"Innate_Immune_Memory\" / \"GSE229940\",\n",
    "}\n",
    "for k, p in REG.items():\n",
    "    assert Path(p).exists(), f\"[MISSING] {k}: {p}\"\n",
    "print(\"[OK] Core NB2 inputs exist.\")\n",
    "\n",
    "\n",
    "# 3) Helpers\n",
    "\n",
    "def normalize_gene_symbols(adata: ad.AnnData) -> ad.AnnData:\n",
    "    adata.var_names = adata.var_names.astype(str).str.strip().str.upper()\n",
    "    adata.var_names_make_unique()\n",
    "    return adata\n",
    "\n",
    "def add_mt_ribo_flags(adata: ad.AnnData) -> ad.AnnData:\n",
    "    \"\"\"Add mitochondrial and ribosomal gene flags.\"\"\"\n",
    "    adata.var_names = adata.var_names.astype(str)\n",
    "    adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n",
    "    adata.var[\"ribo\"] = adata.var_names.str.match(r\"^RP[SL]\\d+\")\n",
    "    return adata\n",
    "\n",
    "def sanitize_for_write(adata: ad.AnnData) -> ad.AnnData:\n",
    "    for df_name in [\"obs\", \"var\"]:\n",
    "        df = getattr(adata, df_name)\n",
    "        if df.index.name is not None:\n",
    "            idx_name = df.index.name\n",
    "            if idx_name in df.columns:\n",
    "                if not pd.Series(df.index, index=df.index).equals(df[idx_name]):\n",
    "                    df.rename(columns={idx_name: f\"{idx_name}_col\"}, inplace=True)\n",
    "            df.index.name = None\n",
    "        df.columns = df.columns.astype(str)\n",
    "    adata.obs_names = adata.obs_names.astype(str)\n",
    "    adata.var_names = adata.var_names.astype(str)\n",
    "    return adata\n",
    "\n",
    "def standardize_obs(adata: ad.AnnData, dataset_id: str, cohort: str) -> ad.AnnData:\n",
    "    adata.obs = adata.obs.copy()\n",
    "    adata.obs[\"dataset_id\"] = dataset_id\n",
    "    adata.obs[\"cohort\"] = cohort\n",
    "    adata.obs_names = adata.obs_names.astype(str)\n",
    "    adata.var_names_make_unique()\n",
    "    return adata\n",
    "\n",
    "\n",
    "# 4) MAD-based adaptive QC\n",
    "\n",
    "def median_abs_deviation(x: np.ndarray) -> float:\n",
    "    return float(np.median(np.abs(x - np.median(x))))\n",
    "\n",
    "def mad_outlier_mask(values: np.ndarray, n_mads: float = 5.0,\n",
    "                     direction: str = \"both\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return boolean mask of outliers based on median ± n_mads × MAD.\n",
    "    direction: 'both', 'upper', 'lower'\n",
    "    \"\"\"\n",
    "    med = np.median(values)\n",
    "    mad = median_abs_deviation(values)\n",
    "    if mad < 1e-6:\n",
    "        return np.zeros(len(values), dtype=bool)\n",
    "    if direction == \"upper\":\n",
    "        return values > med + n_mads * mad\n",
    "    elif direction == \"lower\":\n",
    "        return values < med - n_mads * mad\n",
    "    else:\n",
    "        return (values > med + n_mads * mad) | (values < med - n_mads * mad)\n",
    "\n",
    "def qc_filter_mad(\n",
    "    adata: ad.AnnData,\n",
    "    min_genes: int = 200,\n",
    "    min_cells_per_gene: int = 3,\n",
    "    min_counts: int = 500,\n",
    "    max_mt_pct: float = 20.0,\n",
    "    n_mads_counts: float = 5.0,\n",
    "    n_mads_genes: float = 5.0,\n",
    "    max_counts_hard: Optional[int] = None,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    MAD-based adaptive QC.\n",
    "    Uses median ± N×MAD for UMI counts and gene counts,\n",
    "    with hard floor thresholds as safety nets.\n",
    "    \"\"\"\n",
    "    n0, g0 = adata.n_obs, adata.n_vars\n",
    "    if \"counts\" not in adata.layers:\n",
    "        adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "    if \"mt\" not in adata.var.columns:\n",
    "        adata = add_mt_ribo_flags(adata)\n",
    "\n",
    "    sc.pp.calculate_qc_metrics(\n",
    "        adata, qc_vars=[\"mt\", \"ribo\"], percent_top=None, log1p=False, inplace=True\n",
    "    )\n",
    "\n",
    "    counts = adata.obs[\"total_counts\"].values.astype(float)\n",
    "    genes  = adata.obs[\"n_genes_by_counts\"].values.astype(float)\n",
    "    mt_pct = adata.obs[\"pct_counts_mt\"].values.astype(float)\n",
    "\n",
    "    # MAD-based upper outlier detection for counts\n",
    "    outlier_counts_hi = mad_outlier_mask(np.log1p(counts), n_mads=n_mads_counts, direction=\"upper\")\n",
    "    outlier_genes_hi  = mad_outlier_mask(np.log1p(genes),  n_mads=n_mads_genes,  direction=\"upper\")\n",
    "    outlier_genes_lo  = genes < min_genes\n",
    "    outlier_counts_lo = counts < min_counts\n",
    "    outlier_mt        = mt_pct > max_mt_pct\n",
    "\n",
    "    if max_counts_hard is not None:\n",
    "        outlier_counts_hard = counts > max_counts_hard\n",
    "    else:\n",
    "        outlier_counts_hard = np.zeros(len(counts), dtype=bool)\n",
    "\n",
    "    # Compute effective thresholds for reporting\n",
    "    med_c = np.median(np.log1p(counts))\n",
    "    mad_c = median_abs_deviation(np.log1p(counts))\n",
    "    upper_count_thresh = float(np.expm1(med_c + n_mads_counts * mad_c))\n",
    "\n",
    "    med_g = np.median(np.log1p(genes))\n",
    "    mad_g = median_abs_deviation(np.log1p(genes))\n",
    "    upper_gene_thresh = float(np.expm1(med_g + n_mads_genes * mad_g))\n",
    "\n",
    "    # Store per-cell QC flags\n",
    "    adata.obs[\"outlier_counts_hi\"] = outlier_counts_hi\n",
    "    adata.obs[\"outlier_counts_lo\"] = outlier_counts_lo\n",
    "    adata.obs[\"outlier_genes_hi\"]  = outlier_genes_hi\n",
    "    adata.obs[\"outlier_genes_lo\"]  = outlier_genes_lo\n",
    "    adata.obs[\"outlier_mt\"]        = outlier_mt\n",
    "\n",
    "    keep = ~(outlier_counts_hi | outlier_counts_lo | outlier_counts_hard |\n",
    "             outlier_genes_hi | outlier_genes_lo | outlier_mt)\n",
    "\n",
    "    adata.obs[\"qc_pass\"] = keep\n",
    "\n",
    "    adata_f = adata[keep].copy()\n",
    "    sc.pp.filter_genes(adata_f, min_cells=min_cells_per_gene)\n",
    "\n",
    "    n1, g1 = adata_f.n_obs, adata_f.n_vars\n",
    "\n",
    "    qc_stats = {\n",
    "        \"n_cells_before\": int(n0),\n",
    "        \"n_cells_after\": int(n1),\n",
    "        \"pct_cells_kept\": round(float(n1 / max(n0, 1) * 100.0), 2),\n",
    "        \"n_genes_before\": int(g0),\n",
    "        \"n_genes_after\": int(g1),\n",
    "        \"median_genes\": round(float(np.median(adata_f.obs[\"n_genes_by_counts\"])), 1) if n1 else np.nan,\n",
    "        \"median_umi\": round(float(np.median(adata_f.obs[\"total_counts\"])), 1) if n1 else np.nan,\n",
    "        \"median_mt_pct\": round(float(np.median(adata_f.obs[\"pct_counts_mt\"])), 2) if n1 else np.nan,\n",
    "        \"median_ribo_pct\": round(float(np.median(adata_f.obs[\"pct_counts_ribo\"])), 2) if n1 and \"pct_counts_ribo\" in adata_f.obs.columns else np.nan,\n",
    "        \"n_outlier_counts_hi\": int(outlier_counts_hi.sum()),\n",
    "        \"n_outlier_counts_lo\": int(outlier_counts_lo.sum()),\n",
    "        \"n_outlier_genes_hi\": int(outlier_genes_hi.sum()),\n",
    "        \"n_outlier_genes_lo\": int(outlier_genes_lo.sum()),\n",
    "        \"n_outlier_mt\": int(outlier_mt.sum()),\n",
    "        \"effective_max_counts\": round(upper_count_thresh, 0),\n",
    "        \"effective_max_genes\": round(upper_gene_thresh, 0),\n",
    "        \"n_mads_counts\": float(n_mads_counts),\n",
    "        \"n_mads_genes\": float(n_mads_genes),\n",
    "        \"min_genes\": int(min_genes),\n",
    "        \"min_counts\": int(min_counts),\n",
    "        \"max_mt_pct\": float(max_mt_pct),\n",
    "        \"qc_method\": \"MAD-based adaptive\",\n",
    "    }\n",
    "    return adata_f, qc_stats\n",
    "\n",
    "\n",
    "# 5) Doublet detection (Scrublet)\n",
    "\n",
    "def run_scrublet(adata: ad.AnnData, expected_doublet_rate: float = 0.06,\n",
    "                 seed: int = 42) -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Run Scrublet per sample. Stores doublet_score and predicted_doublet in obs.\n",
    "    Falls back gracefully if too few cells or scrublet unavailable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import scrublet as scr\n",
    "    except ImportError:\n",
    "        print(\"  [WARN] scrublet not installed — skipping doublet detection\")\n",
    "        adata.obs[\"doublet_score\"] = np.nan\n",
    "        adata.obs[\"predicted_doublet\"] = False\n",
    "        return adata\n",
    "\n",
    "    if adata.n_obs < 50:\n",
    "        adata.obs[\"doublet_score\"] = np.nan\n",
    "        adata.obs[\"predicted_doublet\"] = False\n",
    "        return adata\n",
    "\n",
    "    X = adata.layers[\"counts\"] if \"counts\" in adata.layers else adata.X\n",
    "    if sp.issparse(X):\n",
    "        X = X.tocsr()\n",
    "    else:\n",
    "        X = sp.csr_matrix(X)\n",
    "\n",
    "    try:\n",
    "        scrub = scr.Scrublet(X, expected_doublet_rate=expected_doublet_rate,\n",
    "                              random_state=seed)\n",
    "        scores, preds = scrub.scrub_doublets(min_counts=2, min_cells=3,\n",
    "                                              min_gene_variability_pctl=85,\n",
    "                                              n_prin_comps=min(30, adata.n_vars - 1),\n",
    "                                              verbose=False)\n",
    "        adata.obs[\"doublet_score\"] = scores.astype(np.float32)\n",
    "        adata.obs[\"predicted_doublet\"] = preds.astype(bool)\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Scrublet failed: {e}\")\n",
    "        adata.obs[\"doublet_score\"] = np.nan\n",
    "        adata.obs[\"predicted_doublet\"] = False\n",
    "\n",
    "    return adata\n",
    "\n",
    "def run_scrublet_per_sample(adata: ad.AnnData, sample_col: str = \"sample_id\",\n",
    "                            expected_doublet_rate: float = 0.06) -> ad.AnnData:\n",
    "    \"\"\"Run Scrublet independently per sample to avoid cross-sample artifacts.\"\"\"\n",
    "    adata.obs[\"doublet_score\"] = np.nan\n",
    "    adata.obs[\"predicted_doublet\"] = False\n",
    "\n",
    "    if sample_col not in adata.obs.columns:\n",
    "        return run_scrublet(adata, expected_doublet_rate=expected_doublet_rate)\n",
    "\n",
    "    samples = adata.obs[sample_col].unique()\n",
    "    for samp in samples:\n",
    "        mask = adata.obs[sample_col] == samp\n",
    "        sub = adata[mask].copy()\n",
    "        sub = run_scrublet(sub, expected_doublet_rate=expected_doublet_rate)\n",
    "        adata.obs.loc[mask, \"doublet_score\"] = sub.obs[\"doublet_score\"].values\n",
    "        adata.obs.loc[mask, \"predicted_doublet\"] = sub.obs[\"predicted_doublet\"].values\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "# 6) QC snapshot\n",
    "\n",
    "def qc_snapshot(adata: ad.AnnData, dataset_id: str, stage: str,\n",
    "                max_cells: int = 20000) -> pd.DataFrame:\n",
    "    if \"mt\" not in adata.var.columns:\n",
    "        adata = add_mt_ribo_flags(adata)\n",
    "    if \"total_counts\" not in adata.obs.columns:\n",
    "        sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\"],\n",
    "                                   percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "    n = adata.n_obs\n",
    "    if n > max_cells:\n",
    "        idx = np.random.choice(np.arange(n), size=max_cells, replace=False)\n",
    "        obs = adata.obs.iloc[idx]\n",
    "    else:\n",
    "        obs = adata.obs\n",
    "\n",
    "    cols = [\"total_counts\", \"n_genes_by_counts\", \"pct_counts_mt\"]\n",
    "    if \"pct_counts_ribo\" in obs.columns:\n",
    "        cols.append(\"pct_counts_ribo\")\n",
    "    if \"doublet_score\" in obs.columns:\n",
    "        cols.append(\"doublet_score\")\n",
    "\n",
    "    df = obs[cols].copy()\n",
    "    df[\"dataset_id\"] = dataset_id\n",
    "    df[\"stage\"] = stage\n",
    "    return df\n",
    "\n",
    "def per_sample_qc_summary(adata: ad.AnnData, dataset_id: str,\n",
    "                          sample_col: str = \"sample_id\") -> pd.DataFrame:\n",
    "    \"\"\"Per-sample QC summary: median metrics, cell counts, doublet rates.\"\"\"\n",
    "    rows = []\n",
    "    if sample_col not in adata.obs.columns:\n",
    "        samples = [dataset_id]\n",
    "        masks = [np.ones(adata.n_obs, dtype=bool)]\n",
    "    else:\n",
    "        samples = sorted(adata.obs[sample_col].unique())\n",
    "        masks = [adata.obs[sample_col] == s for s in samples]\n",
    "\n",
    "    for samp, mask in zip(samples, masks):\n",
    "        sub = adata.obs.loc[mask]\n",
    "        row = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"sample_id\": samp,\n",
    "            \"n_cells\": int(mask.sum()),\n",
    "            \"median_genes\": round(float(sub[\"n_genes_by_counts\"].median()), 1),\n",
    "            \"median_umi\": round(float(sub[\"total_counts\"].median()), 1),\n",
    "            \"median_mt_pct\": round(float(sub[\"pct_counts_mt\"].median()), 2),\n",
    "        }\n",
    "        if \"pct_counts_ribo\" in sub.columns:\n",
    "            row[\"median_ribo_pct\"] = round(float(sub[\"pct_counts_ribo\"].median()), 2)\n",
    "        if \"doublet_score\" in sub.columns:\n",
    "            ds = sub[\"doublet_score\"].dropna()\n",
    "            row[\"median_doublet_score\"] = round(float(ds.median()), 4) if len(ds) else np.nan\n",
    "        if \"predicted_doublet\" in sub.columns:\n",
    "            pd_col = sub[\"predicted_doublet\"]\n",
    "            n_doub = int(pd_col.sum())\n",
    "            row[\"n_doublets\"] = n_doub\n",
    "            row[\"doublet_rate_pct\"] = round(float(n_doub / max(mask.sum(), 1) * 100), 2)\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# 7) Lavaert robust discovery + loaders\n",
    "\n",
    "def read_tsv_any(path: Path) -> pd.Series:\n",
    "    if path.suffix.lower() == \".gz\":\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None)\n",
    "    return df.iloc[:, 0].astype(str)\n",
    "\n",
    "def read_features_any(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() == \".gz\":\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None)\n",
    "    return df\n",
    "\n",
    "def _sniff_encoding(path: Path, nbytes: int = 4096) -> str:\n",
    "    b = path.read_bytes()[:nbytes]\n",
    "    if b.count(b\"\\x00\") > 100:\n",
    "        return \"utf-16\"\n",
    "    return \"utf-8-sig\"\n",
    "\n",
    "def read_mtx_any(path: Path) -> sp.csr_matrix:\n",
    "    enc = _sniff_encoding(path) if path.suffix.lower() != \".gz\" else \"utf-8\"\n",
    "    if path.suffix.lower() == \".gz\":\n",
    "        import io\n",
    "        with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            txt = f.read()\n",
    "        df = pd.read_csv(io.StringIO(txt), sep=r\"\\s+\", header=None,\n",
    "                         comment=\"%\", engine=\"python\").dropna(how=\"all\")\n",
    "    else:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", header=None, comment=\"%\",\n",
    "                         engine=\"python\", encoding=enc).dropna(how=\"all\")\n",
    "\n",
    "    nrows, ncols, nnz = df.iloc[0, :3].astype(int).values\n",
    "    trip = df.iloc[1:, :3]\n",
    "    i = trip.iloc[:, 0].astype(np.int64).values - 1\n",
    "    j = trip.iloc[:, 1].astype(np.int64).values - 1\n",
    "    x = trip.iloc[:, 2].astype(np.float32).values\n",
    "    return sp.coo_matrix((x, (i, j)), shape=(nrows, ncols)).tocsr()\n",
    "\n",
    "def find_10x_matrix_sets(root: Path):\n",
    "    sets = []\n",
    "    # standard 10x folders\n",
    "    for m in root.rglob(\"matrix.mtx*\"):\n",
    "        folder = m.parent\n",
    "        b1 = folder / \"barcodes.tsv\"\n",
    "        b2 = folder / \"barcodes.tsv.gz\"\n",
    "        f1 = folder / \"features.tsv\"\n",
    "        f2 = folder / \"features.tsv.gz\"\n",
    "        g1 = folder / \"genes.tsv\"\n",
    "        g2 = folder / \"genes.tsv.gz\"\n",
    "        bar = b1 if b1.exists() else (b2 if b2.exists() else None)\n",
    "        feat = f1 if f1.exists() else (f2 if f2.exists() else (g1 if g1.exists() else (g2 if g2.exists() else None)))\n",
    "        if bar and feat:\n",
    "            sets.append({\"matrix\": m, \"barcodes\": bar, \"features\": feat,\n",
    "                         \"folder\": folder, \"prefix\": folder.name})\n",
    "    # flat triples\n",
    "    for m in root.rglob(\"*_matrix.mtx*\"):\n",
    "        folder = m.parent\n",
    "        pref = re.sub(r\"_matrix\\.mtx(\\.gz)?$\", \"\", m.name)\n",
    "        bar = next((p for p in [\n",
    "            folder / f\"{pref}_barcodes.tsv\", folder / f\"{pref}_barcodes.tsv.gz\"\n",
    "        ] if p.exists()), None)\n",
    "        feat = next((p for p in [\n",
    "            folder / f\"{pref}_features.tsv\", folder / f\"{pref}_features.tsv.gz\",\n",
    "            folder / f\"{pref}_genes.tsv\", folder / f\"{pref}_genes.tsv.gz\"\n",
    "        ] if p.exists()), None)\n",
    "        if bar and feat:\n",
    "            sets.append({\"matrix\": m, \"barcodes\": bar, \"features\": feat,\n",
    "                         \"folder\": folder, \"prefix\": pref})\n",
    "    uniq = {}\n",
    "    for s in sets:\n",
    "        uniq[str(Path(s[\"matrix\"]).resolve())] = s\n",
    "    return list(uniq.values())\n",
    "\n",
    "def load_10x_set(s):\n",
    "    obs_names = read_tsv_any(s[\"barcodes\"]).values\n",
    "    gf = read_features_any(s[\"features\"])\n",
    "    var_names = gf.iloc[:, 1].astype(str).values if gf.shape[1] >= 2 else gf.iloc[:, 0].astype(str).values\n",
    "\n",
    "    n_cells = len(obs_names)\n",
    "    n_genes = len(var_names)\n",
    "\n",
    "    M = read_mtx_any(s[\"matrix\"])\n",
    "    sh = M.shape\n",
    "\n",
    "    if sh == (n_genes, n_cells):\n",
    "        X = M.T.tocsr()\n",
    "        orient = \"genes_x_cells__transposed\"\n",
    "    elif sh == (n_cells, n_genes):\n",
    "        X = M.tocsr()\n",
    "        orient = \"cells_x_genes__kept\"\n",
    "    else:\n",
    "        X = M.T.tocsr()\n",
    "        orient = \"mismatch__assumed_genes_x_cells_transposed\"\n",
    "\n",
    "    adata = ad.AnnData(X=X, obs=pd.DataFrame(index=obs_names),\n",
    "                       var=pd.DataFrame(index=var_names))\n",
    "    adata.var_names_make_unique()\n",
    "    dbg = {\n",
    "        \"prefix\": s[\"prefix\"],\n",
    "        \"folder\": str(s[\"folder\"]),\n",
    "        \"matrix\": str(s[\"matrix\"]),\n",
    "        \"barcodes\": str(s[\"barcodes\"]),\n",
    "        \"features\": str(s[\"features\"]),\n",
    "        \"mtx_shape\": str(sh),\n",
    "        \"n_cells_from_barcodes\": int(n_cells),\n",
    "        \"n_genes_from_features\": int(n_genes),\n",
    "        \"orientation\": orient,\n",
    "    }\n",
    "    return adata, dbg\n",
    "\n",
    "\n",
    "# 8) NEGCTRL config auto-lock\n",
    "\n",
    "CONFIG_PATH = OUT_NEGCTRL / \"config_negctrl.json\"\n",
    "CELLTYPE_COL_CANDIDATES = [\n",
    "    \"cell_type\", \"cell_type_label\", \"celltype\", \"annotation\",\n",
    "    \"annotation_fine\", \"annotation_coarse\",\n",
    "]\n",
    "\n",
    "def pick_best_celltype_col(obs: pd.DataFrame) -> Optional[str]:\n",
    "    best, best_score = None, -1\n",
    "    for c in CELLTYPE_COL_CANDIDATES:\n",
    "        if c not in obs.columns:\n",
    "            continue\n",
    "        s = obs[c].astype(str).str.lower()\n",
    "        score = int(s.str.contains(r\"monocyte|macroph|myeloid|dc\\b|dendritic\", regex=True).sum())\n",
    "        if score > best_score:\n",
    "            best_score, best = score, c\n",
    "    return best\n",
    "\n",
    "def load_or_set_negctrl_config(adata_obs: pd.DataFrame) -> dict:\n",
    "    if CONFIG_PATH.exists():\n",
    "        return json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    col = pick_best_celltype_col(adata_obs)\n",
    "    cfg = {\"BLOOD_CELLTYPE_COL\": col}\n",
    "    CONFIG_PATH.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"[NEGCTRL CONFIG SAVED] {CONFIG_PATH} -> {cfg}\")\n",
    "    return cfg\n",
    "\n",
    "def subset_blood_myeloid(adata: ad.AnnData, col: Optional[str]) -> tuple:\n",
    "    if (col is None) or (col not in adata.obs.columns):\n",
    "        return adata, \"UNFILTERED_no_celltype_col_found\"\n",
    "    s = adata.obs[col].astype(str).str.lower()\n",
    "    keep = s.str.contains(r\"monocyte|mono\\b|macroph\", regex=True)\n",
    "    if keep.sum() < 50:\n",
    "        keep = s.str.contains(r\"myeloid|monocyte|mono\\b|macroph|dendritic|dc\\b\", regex=True)\n",
    "    if keep.sum() == 0:\n",
    "        return adata, f\"UNFILTERED_fixed_col={col}_no_match\"\n",
    "    return adata[keep].copy(), f\"FILTERED_fixed_col={col}\"\n",
    "\n",
    "\n",
    "# 9) Batch diagnostic PCA\n",
    "\n",
    "def batch_diagnostic_pca(adata: ad.AnnData, dataset_id: str,\n",
    "                         sample_col: str = \"sample_id\",\n",
    "                         n_hvg: int = 2000, n_pcs: int = 30):\n",
    "    \"\"\"Quick PCA + UMAP colored by sample_id to flag batch effects.\"\"\"\n",
    "    if adata.n_obs < 100:\n",
    "        return None\n",
    "\n",
    "    tmp = adata.copy()\n",
    "    if \"counts\" in tmp.layers:\n",
    "        tmp.X = tmp.layers[\"counts\"].copy()\n",
    "    sc.pp.normalize_total(tmp, target_sum=1e4)\n",
    "    sc.pp.log1p(tmp)\n",
    "\n",
    "    n_hvg_eff = min(n_hvg, tmp.n_vars - 1)\n",
    "    if n_hvg_eff < 200:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        sc.pp.highly_variable_genes(tmp, n_top_genes=n_hvg_eff, flavor=\"seurat_v3\")\n",
    "        tmp = tmp[:, tmp.var[\"highly_variable\"]].copy()\n",
    "        sc.tl.pca(tmp, n_comps=min(n_pcs, tmp.n_vars - 1), svd_solver=\"arpack\")\n",
    "        sc.pp.neighbors(tmp, n_neighbors=15, n_pcs=min(n_pcs, tmp.n_vars - 1))\n",
    "        sc.tl.umap(tmp)\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Batch PCA failed for {dataset_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "# 10) Run loaders\n",
    "\n",
    "TS_H5AD     = Path(REG[\"TS_Thymus_filtered\"])\n",
    "LAVAERT_DIR = Path(REG[\"GSE144870_Lavaert\"])\n",
    "LE_DIR      = Path(REG[\"GSE139042_Le\"])\n",
    "ZENG_DIR    = Path(REG[\"GSE133341_Zeng\"])\n",
    "TS_BLOOD    = Path(REG[\"TS_Blood_NegCtrl\"])\n",
    "\n",
    "qc_rows, deferred = [], []\n",
    "adata_dict, adata_dict_neg = {}, {}\n",
    "qc_snaps = []\n",
    "per_sample_summaries = []\n",
    "lava_dbg_rows = []\n",
    "\n",
    "print(\"\\nAIM 1 / THYMUS: LOADING + QC\")\n",
    "\n",
    "# TS thymus\n",
    "print(\"\\nTS Thymus\")\n",
    "adata_ts = sc.read_h5ad(TS_H5AD)\n",
    "adata_ts = normalize_gene_symbols(adata_ts)\n",
    "adata_ts = add_mt_ribo_flags(adata_ts)\n",
    "adata_ts = standardize_obs(adata_ts, \"TS_Thymus_filtered\", \"thymus\")\n",
    "qc_snaps.append(qc_snapshot(adata_ts, \"TS_Thymus_filtered\", \"pre\"))\n",
    "\n",
    "# Doublet detection\n",
    "print(\"  Running Scrublet ...\")\n",
    "adata_ts = run_scrublet_per_sample(adata_ts, expected_doublet_rate=0.06)\n",
    "n_doub_ts = int(adata_ts.obs[\"predicted_doublet\"].sum())\n",
    "print(f\"  Doublets detected: {n_doub_ts} / {adata_ts.n_obs} \"\n",
    "      f\"({n_doub_ts/max(adata_ts.n_obs,1)*100:.1f}%)\")\n",
    "\n",
    "# Remove doublets before QC\n",
    "adata_ts = adata_ts[~adata_ts.obs[\"predicted_doublet\"]].copy()\n",
    "\n",
    "# MAD-based QC\n",
    "adata_ts, st = qc_filter_mad(adata_ts, max_mt_pct=20.0, max_counts_hard=60000,\n",
    "                              n_mads_counts=5.0, n_mads_genes=5.0)\n",
    "qc_snaps.append(qc_snapshot(adata_ts, \"TS_Thymus_filtered\", \"post\"))\n",
    "st.update({\"dataset_id\": \"TS_Thymus_filtered\", \"source\": \"TabulaSapiens_filtered\",\n",
    "           \"loader\": \"read_h5ad\", \"sample_id\": \"TS\",\n",
    "           \"n_doublets_removed\": n_doub_ts})\n",
    "qc_rows.append(st)\n",
    "per_sample_summaries.append(per_sample_qc_summary(adata_ts, \"TS_Thymus_filtered\"))\n",
    "adata_dict[\"TS_Thymus_filtered\"] = adata_ts\n",
    "print(f\"  TS_Thymus_filtered -> {adata_ts.n_obs:,} cells, {adata_ts.n_vars:,} genes\")\n",
    "\n",
    "# Lavaert\n",
    "print(\"\\nLavaert discovery (recursive):\", LAVAERT_DIR)\n",
    "lava_sets = find_10x_matrix_sets(LAVAERT_DIR)\n",
    "print(f\"  Found {len(lava_sets)} candidate 10x matrix sets under GSE144870.\")\n",
    "for s in lava_sets[:8]:\n",
    "    print(\"   -\", s[\"matrix\"])\n",
    "\n",
    "lava_adatas = []\n",
    "for s in lava_sets:\n",
    "    try:\n",
    "        adata, dbg = load_10x_set(s)\n",
    "        lava_dbg_rows.append(dbg)\n",
    "\n",
    "        adata = normalize_gene_symbols(adata)\n",
    "        adata.obs[\"sample_id\"] = dbg[\"prefix\"]\n",
    "        adata = add_mt_ribo_flags(adata)\n",
    "        adata = standardize_obs(adata, \"GSE144870_Lavaert\", \"thymus\")\n",
    "\n",
    "        qc_snaps.append(qc_snapshot(adata, \"GSE144870_Lavaert\", \"pre\"))\n",
    "\n",
    "        # Doublet detection per sample\n",
    "        adata = run_scrublet(adata, expected_doublet_rate=0.06)\n",
    "        n_d = int(adata.obs[\"predicted_doublet\"].sum())\n",
    "        adata = adata[~adata.obs[\"predicted_doublet\"]].copy()\n",
    "\n",
    "        adata, st = qc_filter_mad(adata, min_genes=150, min_counts=300,\n",
    "                                   max_mt_pct=20.0, n_mads_counts=5.0,\n",
    "                                   n_mads_genes=5.0)\n",
    "        qc_snaps.append(qc_snapshot(adata, \"GSE144870_Lavaert\", \"post\"))\n",
    "\n",
    "        st.update({\"dataset_id\": \"GSE144870_Lavaert\", \"source\": \"GEO\",\n",
    "                   \"loader\": \"find_10x_matrix_sets\", \"sample_id\": dbg[\"prefix\"],\n",
    "                   \"n_doublets_removed\": n_d})\n",
    "        qc_rows.append(st)\n",
    "\n",
    "        if adata.n_obs > 0:\n",
    "            lava_adatas.append(adata)\n",
    "        else:\n",
    "            deferred.append({\"file\": dbg[\"matrix\"], \"reason\": \"0 cells after QC\"})\n",
    "    except Exception as e:\n",
    "        deferred.append({\"file\": str(s[\"matrix\"]), \"reason\": str(e)})\n",
    "        qc_rows.append({\"dataset_id\": \"GSE144870_Lavaert\", \"source\": \"GEO\",\n",
    "                        \"loader\": \"find_10x_matrix_sets\",\n",
    "                        \"sample_id\": s.get(\"prefix\", \"\"), \"error\": str(e)})\n",
    "\n",
    "df_lava_dbg = pd.DataFrame(lava_dbg_rows)\n",
    "\n",
    "if lava_adatas:\n",
    "    adata_lava = ad.concat(lava_adatas, join=\"outer\", merge=\"same\")\n",
    "    adata_lava = standardize_obs(adata_lava, \"GSE144870_Lavaert\", \"thymus\")\n",
    "    per_sample_summaries.append(per_sample_qc_summary(adata_lava, \"GSE144870_Lavaert\"))\n",
    "    adata_dict[\"GSE144870_Lavaert\"] = adata_lava\n",
    "    print(f\"  Lavaert merged -> {adata_lava.n_obs:,} cells, {adata_lava.n_vars:,} genes\")\n",
    "else:\n",
    "    print(\"  [WARN] Lavaert still not loaded. See Lavaert_Debug.\")\n",
    "\n",
    "# Le\n",
    "print(\"\\nLe GSE139042\")\n",
    "le_files = sorted(list(LE_DIR.glob(\"GSM*Thymus*counts*.txt\")) +\n",
    "                  sorted(list(LE_DIR.glob(\"GSM*Thymus*counts*.csv\"))))\n",
    "le_loaded = []\n",
    "for f in le_files:\n",
    "    try:\n",
    "        sep = \"\\t\" if f.suffix.lower() == \".txt\" else \",\"\n",
    "        df = pd.read_csv(f, sep=sep, index_col=0)\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        X = sp.csr_matrix(df.T.values.astype(np.float32))\n",
    "        adata = ad.AnnData(X=X, obs=pd.DataFrame(index=df.columns.astype(str)),\n",
    "                           var=pd.DataFrame(index=df.index.astype(str)))\n",
    "        adata = normalize_gene_symbols(adata)\n",
    "        adata.obs[\"sample_id\"] = f.stem\n",
    "        adata = add_mt_ribo_flags(adata)\n",
    "        adata = standardize_obs(adata, \"GSE139042_Le\", \"thymus\")\n",
    "\n",
    "        qc_snaps.append(qc_snapshot(adata, \"GSE139042_Le\", \"pre\"))\n",
    "\n",
    "        # Doublet detection\n",
    "        adata = run_scrublet(adata, expected_doublet_rate=0.06)\n",
    "        n_d = int(adata.obs[\"predicted_doublet\"].sum())\n",
    "        adata = adata[~adata.obs[\"predicted_doublet\"]].copy()\n",
    "\n",
    "        adata, st = qc_filter_mad(adata, max_mt_pct=20.0, n_mads_counts=5.0,\n",
    "                                   n_mads_genes=5.0)\n",
    "        qc_snaps.append(qc_snapshot(adata, \"GSE139042_Le\", \"post\"))\n",
    "\n",
    "        st.update({\"dataset_id\": \"GSE139042_Le\", \"source\": \"GEO\",\n",
    "                   \"loader\": \"txt/csv_dense_to_sparse_numeric\", \"sample_id\": f.stem,\n",
    "                   \"n_doublets_removed\": n_d})\n",
    "        qc_rows.append(st)\n",
    "\n",
    "        if adata.n_obs > 0:\n",
    "            le_loaded.append(adata)\n",
    "        else:\n",
    "            deferred.append({\"file\": str(f), \"reason\": \"0 cells after QC\"})\n",
    "    except Exception as e:\n",
    "        deferred.append({\"file\": str(f), \"reason\": str(e)})\n",
    "        qc_rows.append({\"dataset_id\": \"GSE139042_Le\", \"source\": \"GEO\",\n",
    "                        \"loader\": \"txt/csv_dense_to_sparse_numeric\",\n",
    "                        \"sample_id\": f.stem, \"error\": str(e)})\n",
    "\n",
    "if le_loaded:\n",
    "    adata_le = ad.concat(le_loaded, join=\"outer\", merge=\"same\")\n",
    "    adata_le = standardize_obs(adata_le, \"GSE139042_Le\", \"thymus\")\n",
    "    per_sample_summaries.append(per_sample_qc_summary(adata_le, \"GSE139042_Le\"))\n",
    "    adata_dict[\"GSE139042_Le\"] = adata_le\n",
    "    print(f\"  Le merged -> {adata_le.n_obs:,} cells, {adata_le.n_vars:,} genes\")\n",
    "\n",
    "# Zeng — documented skip with justification\n",
    "print(\"\\nZeng GSE133341\")\n",
    "zeng_candidates = sorted([p for p in ZENG_DIR.glob(\"GSM*.txt\")\n",
    "                          if \"thymus\" in p.name.lower()])\n",
    "for f in zeng_candidates:\n",
    "    deferred.append({\"file\": str(f),\n",
    "                     \"reason\": \"Zeng GSE133341: excluded — bulk RNA-seq or \"\n",
    "                               \"non-standard gene×cell format incompatible \"\n",
    "                               \"with single-cell QC pipeline. Not a data loss; \"\n",
    "                               \"3 independent thymus cohorts remain (TS, Lavaert, Le).\"})\n",
    "print(\"  [INFO] Zeng excluded (documented justification in deferred table).\")\n",
    "\n",
    "# NEGCTRL\n",
    "print(\"\\nNEGCTRL\")\n",
    "adata_blood = sc.read_h5ad(TS_BLOOD)\n",
    "adata_blood = normalize_gene_symbols(adata_blood)\n",
    "adata_blood = add_mt_ribo_flags(adata_blood)\n",
    "adata_blood = standardize_obs(adata_blood, \"TS_Blood_NegCtrl\", \"negctrl_peripheral\")\n",
    "\n",
    "cfg = load_or_set_negctrl_config(adata_blood.obs)\n",
    "col = cfg.get(\"BLOOD_CELLTYPE_COL\", None)\n",
    "adata_blood_my, flag = subset_blood_myeloid(adata_blood, col)\n",
    "adata_blood_my.obs[\"negctrl_filter_flag\"] = flag\n",
    "\n",
    "qc_snaps.append(qc_snapshot(adata_blood_my, \"TS_Blood_NegCtrl\", \"pre\"))\n",
    "\n",
    "# Doublet detection\n",
    "adata_blood_my = run_scrublet_per_sample(adata_blood_my, expected_doublet_rate=0.06)\n",
    "n_doub_blood = int(adata_blood_my.obs[\"predicted_doublet\"].sum())\n",
    "adata_blood_my = adata_blood_my[~adata_blood_my.obs[\"predicted_doublet\"]].copy()\n",
    "\n",
    "adata_blood_my, st_b = qc_filter_mad(adata_blood_my, min_genes=150, min_counts=300,\n",
    "                                      max_mt_pct=25.0, n_mads_counts=5.0,\n",
    "                                      n_mads_genes=5.0)\n",
    "qc_snaps.append(qc_snapshot(adata_blood_my, \"TS_Blood_NegCtrl\", \"post\"))\n",
    "st_b.update({\"dataset_id\": \"TS_Blood_NegCtrl\", \"source\": \"TabulaSapiens_Blood\",\n",
    "             \"loader\": \"read_h5ad\", \"sample_id\": flag,\n",
    "             \"n_doublets_removed\": n_doub_blood})\n",
    "qc_rows.append(st_b)\n",
    "per_sample_summaries.append(per_sample_qc_summary(adata_blood_my, \"TS_Blood_NegCtrl\"))\n",
    "adata_dict_neg[\"TS_Blood_NegCtrl_Myeloid\"] = adata_blood_my\n",
    "print(f\"  Blood negctrl -> {adata_blood_my.n_obs:,} cells, {adata_blood_my.n_vars:,} genes, \"\n",
    "      f\"flag={flag}, col={col}, doublets_removed={n_doub_blood}\")\n",
    "\n",
    "# Innate memory stats\n",
    "gse229940 = next((p for p in [\n",
    "    Path(REG[\"GSE229940_dir\"]) / \"GSE229940_countmtx.csv\",\n",
    "    Path(REG[\"GSE229940_dir\"]) / \"GSE229940_countmtx.csv.gz\",\n",
    "] if p.exists()), None)\n",
    "\n",
    "imm_rows = []\n",
    "if gse229940:\n",
    "    head = pd.read_csv(gse229940, nrows=5) if gse229940.suffix.lower() != \".gz\" else pd.read_csv(gse229940, compression=\"gzip\", nrows=5)\n",
    "    imm_rows.append({\"file\": str(gse229940), \"cols\": int(head.shape[1]),\n",
    "                     \"note\": \"innate memory anchor\"})\n",
    "df_imm = pd.DataFrame(imm_rows)\n",
    "df_imm.to_csv(OUT_IMM / \"GSE229940_manifest.csv\", index=False)\n",
    "\n",
    "\n",
    "# 11) Save processed h5ad\n",
    "\n",
    "saved_rows = []\n",
    "for ds_id, adata_ in adata_dict.items():\n",
    "    if adata_.n_obs == 0:\n",
    "        continue\n",
    "    adata_ = sanitize_for_write(adata_)\n",
    "    out = OUT_AIM1 / f\"{ds_id}__qc.h5ad\"\n",
    "    adata_.write_h5ad(out)\n",
    "    saved_rows.append({\"dataset_id\": ds_id, \"path\": str(out),\n",
    "                       \"n_cells\": int(adata_.n_obs), \"n_genes\": int(adata_.n_vars)})\n",
    "\n",
    "for ds_id, adata_ in adata_dict_neg.items():\n",
    "    if adata_.n_obs == 0:\n",
    "        continue\n",
    "    adata_ = sanitize_for_write(adata_)\n",
    "    out = OUT_NEGCTRL / f\"{ds_id}__qc.h5ad\"\n",
    "    adata_.write_h5ad(out)\n",
    "    saved_rows.append({\"dataset_id\": ds_id, \"path\": str(out),\n",
    "                       \"n_cells\": int(adata_.n_obs), \"n_genes\": int(adata_.n_vars)})\n",
    "\n",
    "df_saved = pd.DataFrame(saved_rows)\n",
    "df_qc = pd.DataFrame(qc_rows)\n",
    "df_def = pd.DataFrame(deferred)\n",
    "df_snaps = pd.concat(qc_snaps, ignore_index=True) if qc_snaps else pd.DataFrame()\n",
    "df_per_sample = pd.concat(per_sample_summaries, ignore_index=True) if per_sample_summaries else pd.DataFrame()\n",
    "\n",
    "\n",
    "# 12) Outlier sample flagging\n",
    "\n",
    "def flag_outlier_samples(df_ps: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Flag samples that are statistical outliers across QC metrics.\"\"\"\n",
    "    if df_ps.empty:\n",
    "        return df_ps\n",
    "    df_ps = df_ps.copy()\n",
    "    for metric in [\"median_genes\", \"median_umi\", \"median_mt_pct\"]:\n",
    "        if metric not in df_ps.columns:\n",
    "            continue\n",
    "        vals = df_ps[metric].dropna().values\n",
    "        if len(vals) < 3:\n",
    "            df_ps[f\"outlier_{metric}\"] = False\n",
    "            continue\n",
    "        med = np.median(vals)\n",
    "        mad = median_abs_deviation(vals)\n",
    "        if mad < 1e-6:\n",
    "            df_ps[f\"outlier_{metric}\"] = False\n",
    "        else:\n",
    "            df_ps[f\"outlier_{metric}\"] = np.abs(df_ps[metric].values - med) > 3 * mad\n",
    "\n",
    "    outlier_cols = [c for c in df_ps.columns if c.startswith(\"outlier_\")]\n",
    "    if outlier_cols:\n",
    "        df_ps[\"any_outlier\"] = df_ps[outlier_cols].any(axis=1)\n",
    "    return df_ps\n",
    "\n",
    "df_per_sample = flag_outlier_samples(df_per_sample)\n",
    "\n",
    "\n",
    "# 13) QC figures\n",
    "\n",
    "print(\"\\nGENERATING QC FIGURES\")\n",
    "\n",
    "plot_order = [\"TS_Thymus_filtered\", \"GSE144870_Lavaert\", \"GSE139042_Le\", \"TS_Blood_NegCtrl\"]\n",
    "\n",
    "# Fig S2a-c: QC distributions pre/post\n",
    "if not df_snaps.empty:\n",
    "    df_snaps_plot = df_snaps[df_snaps[\"dataset_id\"].isin(plot_order)].copy()\n",
    "\n",
    "    for metric, ylabel in [(\"total_counts\", \"UMI\"), (\"n_genes_by_counts\", \"Genes\"),\n",
    "                           (\"pct_counts_mt\", \"MT%\")]:\n",
    "        fig = plt.figure(figsize=(8.6, 3.9))\n",
    "        ax = fig.add_subplot(111)\n",
    "        groups, xticks = [], []\n",
    "        for ds in plot_order:\n",
    "            for stage in [\"pre\", \"post\"]:\n",
    "                sub = df_snaps_plot[(df_snaps_plot[\"dataset_id\"] == ds) &\n",
    "                                   (df_snaps_plot[\"stage\"] == stage)][metric].dropna().values\n",
    "                if len(sub) == 0:\n",
    "                    continue\n",
    "                groups.append(sub)\n",
    "                xticks.append(f\"{ds}\\n{stage}\")\n",
    "        if groups:\n",
    "            parts = ax.violinplot(groups, showmedians=True, showextrema=False)\n",
    "            for i, pc in enumerate(parts[\"bodies\"]):\n",
    "                color = PALETTE_STAGE[0] if \"pre\" in xticks[i] else PALETTE_STAGE[1]\n",
    "                pc.set_facecolor(color)\n",
    "                pc.set_alpha(0.6)\n",
    "            parts[\"cmedians\"].set_color(\"black\")\n",
    "            ax.set_xticks(range(1, len(xticks) + 1))\n",
    "            ax.set_xticklabels(xticks, rotation=45, ha=\"right\")\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_title(f\"QC distribution pre vs post: {ylabel}\")\n",
    "            ax.grid(axis=\"y\", alpha=0.2)\n",
    "        plt.tight_layout()\n",
    "        save_fig(fig, f\"Fig2_QC_{metric}_PrePost\", kind=\"Supplementary\")\n",
    "\n",
    "# Fig S2d: Cells per sample barplot (colored by dataset, legend)\n",
    "if not df_per_sample.empty:\n",
    "    fig = plt.figure(figsize=(9.0, 3.8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    df_bar = df_per_sample.sort_values([\"dataset_id\", \"sample_id\"])\n",
    "    x = np.arange(len(df_bar))\n",
    "\n",
    "    # Short dataset aliases for legend + color mapping\n",
    "    _ds_alias = {\"TS_Thymus_filtered\": \"TS Thymus\", \"GSE144870_Lavaert\": \"Lavaert\",\n",
    "                 \"GSE139042_Le\": \"Le\", \"TS_Blood_NegCtrl\": \"Blood NegCtrl\"}\n",
    "    _ds_palette = {\"TS_Thymus_filtered\": \"#1b9e77\", \"GSE144870_Lavaert\": \"#d95f02\",\n",
    "                   \"GSE139042_Le\": \"#7570b3\", \"TS_Blood_NegCtrl\": \"#e7298a\"}\n",
    "\n",
    "    # Clean sample labels: strip GSM prefix junk, drop _matrix/_barcodes etc.\n",
    "    def _clean_sample(sid, dsid):\n",
    "        s = str(sid)\n",
    "        s = re.sub(r\"^GSM\\d+_?\", \"\", s)           # strip GSM ID prefix\n",
    "        s = re.sub(r\"_?(matrix|barcodes|features|filtered).*\", \"\", s, flags=re.I)\n",
    "        s = s.replace(\"_\", \" \").strip()\n",
    "        if not s:\n",
    "            s = str(sid)[:15]                       # fallback: truncated original\n",
    "        return s\n",
    "\n",
    "    bar_colors = [_ds_palette.get(r[\"dataset_id\"], \"#999999\") for _, r in df_bar.iterrows()]\n",
    "    # Dim outlier bars\n",
    "    bar_edge = [\"#de2d26\" if r.get(\"any_outlier\", False) else \"white\"\n",
    "                for _, r in df_bar.iterrows()]\n",
    "    bar_lw = [1.5 if r.get(\"any_outlier\", False) else 0.3\n",
    "              for _, r in df_bar.iterrows()]\n",
    "\n",
    "    ax.bar(x, df_bar[\"n_cells\"].values, color=bar_colors, edgecolor=bar_edge,\n",
    "           linewidth=bar_lw)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([_clean_sample(r[\"sample_id\"], r[\"dataset_id\"])\n",
    "                        for _, r in df_bar.iterrows()],\n",
    "                       rotation=90, fontsize=3.5)\n",
    "    ax.set_ylabel(\"Cells after QC\")\n",
    "    ax.set_title(\"Cells per sample (red border = outlier)\")\n",
    "    ax.grid(axis=\"y\", alpha=0.2)\n",
    "\n",
    "    # Legend by dataset\n",
    "    seen = {}\n",
    "    for _, r in df_bar.iterrows():\n",
    "        ds = r[\"dataset_id\"]\n",
    "        if ds not in seen:\n",
    "            seen[ds] = _ds_palette.get(ds, \"#999999\")\n",
    "    handles = [mpatches.Patch(color=c, label=_ds_alias.get(ds, ds))\n",
    "               for ds, c in seen.items()]\n",
    "    ax.legend(handles=handles, fontsize=5.5, frameon=False, loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_fig(fig, \"Fig2_QC_CellsPerSample\", kind=\"Supplementary\")\n",
    "\n",
    "# Fig S2e: Scatter genes vs UMI colored by MT%\n",
    "for ds_id, adata_ in adata_dict.items():\n",
    "    if adata_.n_obs < 50 or \"total_counts\" not in adata_.obs.columns:\n",
    "        continue\n",
    "    fig = plt.figure(figsize=(5.5, 4.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    n_plot = min(adata_.n_obs, 15000)\n",
    "    idx = np.random.choice(adata_.n_obs, size=n_plot, replace=False)\n",
    "    obs_sub = adata_.obs.iloc[idx]\n",
    "    sc_plot = ax.scatter(\n",
    "        obs_sub[\"total_counts\"], obs_sub[\"n_genes_by_counts\"],\n",
    "        c=obs_sub[\"pct_counts_mt\"], cmap=\"RdYlGn_r\", s=2, alpha=0.5,\n",
    "        vmin=0, vmax=min(20, obs_sub[\"pct_counts_mt\"].quantile(0.99)),\n",
    "        rasterized=True\n",
    "    )\n",
    "    ax.set_xlabel(\"Total UMI counts\")\n",
    "    ax.set_ylabel(\"Genes detected\")\n",
    "    ax.set_title(f\"{ds_id}: genes vs UMI (color = MT%)\")\n",
    "    cbar = plt.colorbar(sc_plot, ax=ax, fraction=0.03, pad=0.02)\n",
    "    cbar.set_label(\"MT%\")\n",
    "    plt.tight_layout()\n",
    "    save_fig(fig, f\"Fig2_QC_scatter_{ds_id}\", kind=\"Supplementary\")\n",
    "\n",
    "# Fig S2f: Doublet score distribution per dataset\n",
    "doublet_data = []\n",
    "for ds_id, adata_ in {**adata_dict, **adata_dict_neg}.items():\n",
    "    if \"doublet_score\" in adata_.obs.columns:\n",
    "        scores = adata_.obs[\"doublet_score\"].dropna().values\n",
    "        if len(scores) > 0:\n",
    "            for s in scores[:10000]:\n",
    "                doublet_data.append({\"dataset\": ds_id, \"doublet_score\": s})\n",
    "\n",
    "if doublet_data:\n",
    "    df_doub_plot = pd.DataFrame(doublet_data)\n",
    "    datasets = df_doub_plot[\"dataset\"].unique()\n",
    "    fig, axes = plt.subplots(1, len(datasets), figsize=(3.5 * len(datasets), 3.2),\n",
    "                              constrained_layout=True, squeeze=False)\n",
    "    for i, ds in enumerate(datasets):\n",
    "        ax = axes[0, i]\n",
    "        vals = df_doub_plot[df_doub_plot[\"dataset\"] == ds][\"doublet_score\"].values\n",
    "        ax.hist(vals, bins=50, color=\"#636363\", edgecolor=\"white\", linewidth=0.3,\n",
    "                density=True)\n",
    "        ax.set_xlabel(\"Doublet score\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(ds, fontsize=7)\n",
    "        ax.grid(alpha=0.15)\n",
    "    save_fig(fig, \"Fig2_QC_DoubletScores\", kind=\"Supplementary\")\n",
    "\n",
    "# Fig S2g: Batch diagnostic PCA/UMAP per dataset\n",
    "print(\"\\nBATCH DIAGNOSTIC PCA\")\n",
    "for ds_id, adata_ in adata_dict.items():\n",
    "    if adata_.n_obs < 200:\n",
    "        continue\n",
    "    sample_col = \"sample_id\"\n",
    "    if sample_col not in adata_.obs.columns:\n",
    "        continue\n",
    "    n_samples = adata_.obs[sample_col].nunique()\n",
    "    if n_samples < 2:\n",
    "        continue\n",
    "\n",
    "    print(f\"  Batch PCA for {ds_id} ({n_samples} samples) ...\")\n",
    "    tmp = batch_diagnostic_pca(adata_, ds_id, sample_col=sample_col)\n",
    "    if tmp is None:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9.0, 3.8), constrained_layout=True)\n",
    "\n",
    "    # PCA\n",
    "    ax = axes[0]\n",
    "    pca_xy = tmp.obsm[\"X_pca\"][:, :2]\n",
    "    samples = tmp.obs[sample_col].astype(\"category\")\n",
    "    palette = sns.color_palette(\"Set2\", n_colors=samples.cat.categories.size)\n",
    "    for idx_s, samp in enumerate(samples.cat.categories):\n",
    "        mask = (samples == samp).values\n",
    "        ax.scatter(pca_xy[mask, 0], pca_xy[mask, 1], s=2, alpha=0.4,\n",
    "                   color=palette[idx_s % len(palette)], label=samp,\n",
    "                   rasterized=True)\n",
    "    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(f\"{ds_id}: PCA by sample\")\n",
    "    if n_samples <= 10:\n",
    "        ax.legend(markerscale=4, fontsize=5, frameon=False,\n",
    "                  bbox_to_anchor=(1.01, 1.0), loc=\"upper left\")\n",
    "\n",
    "    # UMAP\n",
    "    ax = axes[1]\n",
    "    umap_xy = tmp.obsm[\"X_umap\"]\n",
    "    for idx_s, samp in enumerate(samples.cat.categories):\n",
    "        mask = (samples == samp).values\n",
    "        ax.scatter(umap_xy[mask, 0], umap_xy[mask, 1], s=2, alpha=0.4,\n",
    "                   color=palette[idx_s % len(palette)], label=samp,\n",
    "                   rasterized=True)\n",
    "    ax.set_xlabel(\"UMAP1\"); ax.set_ylabel(\"UMAP2\")\n",
    "    ax.set_title(f\"{ds_id}: UMAP by sample\")\n",
    "\n",
    "    save_fig(fig, f\"Fig2_QC_BatchPCA_{ds_id}\", kind=\"Supplementary\")\n",
    "\n",
    "\n",
    "# 14) Table 2\n",
    "\n",
    "save_table_xlsx(\n",
    "    sheets={\n",
    "        \"Aim1_Thymus_QC\": df_qc,\n",
    "        \"Per_Sample_QC_Summary\": df_per_sample,\n",
    "        \"Saved_Outputs\": df_saved,\n",
    "        \"Deferred_or_Skipped\": df_def,\n",
    "        \"Lavaert_Debug\": df_lava_dbg,\n",
    "        \"InnateMemory_Stats\": df_imm,\n",
    "    },\n",
    "    fname=\"Table2\",\n",
    "    kind=\"Supplementary\"\n",
    ")\n",
    "\n",
    "\n",
    "# 15) Final report\n",
    "\n",
    "print(\"\\nNB2 DONE\")\n",
    "print(\"NEGCTRL config:\", CONFIG_PATH)\n",
    "print(f\"\"\"\n",
    "OUTPUTS:\n",
    "  h5ad:\n",
    "    - {OUT_AIM1}/TS_Thymus_filtered__qc.h5ad\n",
    "    - {OUT_AIM1}/GSE144870_Lavaert__qc.h5ad\n",
    "    - {OUT_AIM1}/GSE139042_Le__qc.h5ad\n",
    "    - {OUT_NEGCTRL}/TS_Blood_NegCtrl_Myeloid__qc.h5ad\n",
    "  Tables:\n",
    "    - Supplementary_Table2.xlsx (expanded: Per_Sample_QC_Summary sheet added)\n",
    "  Figures:\n",
    "    - Supplementary_Fig2_QC_total_counts_PrePost.png\n",
    "    - Supplementary_Fig2_QC_n_genes_by_counts_PrePost.png\n",
    "    - Supplementary_Fig2_QC_pct_counts_mt_PrePost.png\n",
    "    - Supplementary_Fig2_QC_CellsPerSample.png (new)\n",
    "    - Supplementary_Fig2_QC_scatter_*.png (new, per dataset)\n",
    "    - Supplementary_Fig2_QC_DoubletScores.png (new)\n",
    "    - Supplementary_Fig2_QC_BatchPCA_*.png (new, per dataset)\n",
    "\"\"\")"
   ]
  }
 ]
}